@article{hofmann.2010,
author = {Marc Hofmann, Cristian Gatu and Erricos John Kontoghiorghes},
title = {An Exact Least Trimmed Squares Algorithm for a Range of Coverage Values},
JOURNAL = {J. Comput. Graph. Stat.},
  FJOURNAL = {Journal of Computational and Graphical Statistics},
volume = {19},
number = {1},
pages = {191--204},
year = {2010},
publisher = {ASA Website},
doi = {10.1198/jcgs.2009.07091},
URL = {https://doi.org/10.1198/jcgs.2009.07091},
eprint = {https://doi.org/10.1198/jcgs.2009.07091}
}

@book {efron_hastie.2021,
    AUTHOR = {Efron, Bradley and Hastie, Trevor},
     TITLE = {Computer {A}ge {S}tatistical {I}nference---{A}lgorithms, {E}vidence, and
              {D}ata {S}cience},
   PUBLISHER = {Cambridge University Press, Cambridge},
      YEAR = {2021},
     PAGES = {xix+491},
      ISBN = {978-1-108-82341-8},
   MRCLASS = {62-02 (62-08 62Fxx 62M45 65Gxx)},
  MRNUMBER = {4412544},
}

@book {efron_hastie.2016,
    AUTHOR = {Efron, Bradley and Hastie, Trevor},
     TITLE = {Computer age statistical inference},
    VOLUME = {5},
      NOTE = {Algorithms, evidence, and data science},
 PUBLISHER = {Cambridge University Press, New York},
      YEAR = {2016},
     PAGES = {xix+475},
      ISBN = {978-1-107-14989-2},
   MRCLASS = {62-02 (62F15 62F40 62G08 62H30 62J07 62Jxx 62N01)},
  MRNUMBER = {3523956},
MRREVIEWER = {Pierre\ Alquier},
       DOI = {10.1017/CBO9781316576533},
       URL = {https://doi.org/10.1017/CBO9781316576533},
}

 @Manual{scott.2023,
    title = {BoomSpikeSlab: MCMC for Spike and Slab Regression},
    author = {Steven L. Scott},
    year = {2023},
    note = {R package version 1.2.6},
    url = {https://CRAN.R-project.org/package=BoomSpikeSlab},
  }

@article{colombo.2014,
  title={Order-independent constraint-based causal structure learning},
  author={Diego Colombo and Marloes H. Maathuis},
  journal={J. Mach. Learn. Res.},
  year={2014},
  volume={15},
  pages={3921-3962},
  url={https://api.semanticscholar.org/CorpusID:16836598}
  }

@book{spirtes.2000,
  added-at = {2009-09-12T19:19:34.000+0200},
  author = {Spirtes, {Peter} and Glymour, {Clark}  and Scheines, {Richard}},
  edition = {2nd},
   keywords = {imported},
  owner = {Mozaherul Hoque},
  publisher = {MIT press},
  review = {PC algorithm},
  timestamp = {2009-09-12T19:19:43.000+0200},
  title = {Causation, Prediction, and Search},
  year = 2000
}

@ARTICLE{martino-2018,
       author = {{Martino}, Luca},
        title = "{A Review of Multiple Try MCMC algorithms for Signal Processing}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Computation, Statistics - Machine Learning},
         year = "2018",
        month = "Jan",
          eid = {arXiv:1801.09065},
        pages = {arXiv:1801.09065},
archivePrefix = {arXiv},
       eprint = {1801.09065},
 primaryClass = {stat.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018arXiv180109065M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{casarin-2010,
       author = {{Casarin}, Roberto and {Craiu}, Radu V. and {Leisen}, Fabrizio},
        title = "{Interacting Multiple Try Algorithms with Different Proposal Distributions}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Computation},
         year = "2010",
        month = "Nov",
          eid = {arXiv:1011.1170},
        pages = {arXiv:1011.1170},
archivePrefix = {arXiv},
       eprint = {1011.1170},
 primaryClass = {stat.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2010arXiv1011.1170C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{demetris-2009,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/25651265},
 abstract = {Model search in probit regression is often conducted by simultaneously exploring the model and parameter space, using a reversible jump MCMC sampler. Standard samplers often have low model acceptance probabilities when there are many more regressors than observations. Implementing recent suggestions in the literature leads to much higher acceptance rates. However, high acceptance rates are often associated with poor mixing of chains. Thus, we design a more general model proposal that allows us to propose models "further" from our current model. This proposal can be tuned to achieve a suitable acceptance rate for good mixing. The effectiveness of this proposal is linked to the form of the marginalization scheme when updating the model and we propose a new efficient implementation of the automatic generic transdimensional algorithm of Green (2003). We also implement other previously proposed samplers and compare the efficiency of all methods on some gene expression datasets. Finally, the results of these applications lead us to propose guidelines for choosing between samplers. Relevant code and datasets are posted as an online supplement.},
 author = {Demetris Lamnisos and Jim E. Griffin and Mark F. J. Steel},
JOURNAL = {J. Comput. Graph. Stat.},
  FJOURNAL = {Journal of Computational and Graphical Statistics},
 number = {3},
 pages = {592--612},
 publisher = {[American Statistical Association, Taylor \&  Francis, Ltd.]},
 title = {Transdimensional Sampling Algorithms for Bayesian Variable Selection in Classification Problems With Many More Variables Than Observations},
 volume = {18},
 year = {2009}
}

@article{pandolfi-2010,
author = {Pandolfi, Silvia and Bartolucci, Francesco and Friel, Nial},
year = {2010},
month = {01},
pages = {581-588},
title = {A generalization of the Multiple-try Metropolis algorithm for Bayesian estimation and model selection.},
volume = {9},
journal = {Journal of Machine Learning Research - Proceedings Track}
}

@ARTICLE{polson-2012,
       author = {{Polson}, Nicholas G. and {Scott}, James G. and {Windle}, Jesse},
        title = "{Bayesian inference for logistic models using Polya-Gamma latent variables}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Methodology, Statistics - Computation, Statistics - Machine Learning},
         year = "2012",
        month = "May",
          eid = {arXiv:1205.0310},
        pages = {arXiv:1205.0310},
archivePrefix = {arXiv},
       eprint = {1205.0310},
 primaryClass = {stat.ME},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2012arXiv1205.0310P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{papathomas-2009,
author = {Papathomas, Michail and Dellaportas, Petros and Vasdekis, Vassilis},
year = {2009},
month = {01},
pages = {},
title = {A general proposal construction for reversible jump MCMC}
}

@ARTICLE{green-1995,
    author = {Peter J. Green},
    title = {Reversible jump Markov chain Monte Carlo computation and Bayesian model determination},
    journal = {Biometrika},
    year = {1995},
    volume = {82},
    pages = {711--732}
}

@article{liu-1994,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2337047},
 abstract = {We study the covariance structure of a Markov chain generated by the Gibbs sampler, with emphasis on data augmentation. When applied to a Bayesian missing data problem, the Gibbs sampler produces two natural approximations for the posterior distribution of the parameter vector: the empirical distribution based on the sampled values of the parameter vector, and a mixture of complete data posteriors. We prove that Rao-Blackwellization causes a one-lag delay for the autocovariances among dependent samples obtained from data augmentation, and consequently, the mixture approximation produces estimates with smaller variances than the empirical approximation. The covariance structure results are used to compare different augmentation schemes. It is shown that collapsing and grouping random components in a Gibbs sampler with two or three components usually result in more efficient sampling schemes.},
 author = {Jun S. Liu and Wing Hung Wong and Augustine Kong},
 journal = {Biometrika},
 number = {1},
 pages = {27--40},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Covariance Structure of the Gibbs Sampler with Applications to the Comparisons of Estimators and Augmentation Schemes},
 volume = {81},
 year = {1994}
}

@ARTICLE{sisson-2010,
       author = {{Fan}, Y and {Sisson}, S A},
        title = "{Reversible jump Markov chain Monte Carlo}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Methodology},
         year = "2010",
        month = "Jan",
          eid = {arXiv:1001.2055},
        pages = {arXiv:1001.2055},
archivePrefix = {arXiv},
       eprint = {1001.2055},
 primaryClass = {stat.ME},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2010arXiv1001.2055F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{holmes-2006,
          volume = {1},
          number = {1},
          author = {C C Holmes and L Held},
           title = {Bayesian auxiliary variable models for binary and multinomial regression},
       publisher = {International Society for Bayesian Analysis},
         journal = {Bayesian Analysis},
           pages = {145--168},
            year = {2006},
             url = {https://doi.org/10.5167/uzh-36451},
            issn = {1931-6690},
             doi = {10.1214/06-BA105}
}

@article{mcculloch-1997,
 ISSN = {10170405, 19968507},
 URL = {http://www.jstor.org/stable/24306083},
 abstract = {This paper describes and compares various hierarchical mixture prior formulations of variable selection uncertainty in normal linear regression models. These include the nonconjugate SSVS formulation of George and McCulloch (1993), as well as conjugate formulations which allow for analytical simplification. Hyperparameter settings which base selection on practical significance, and the implications of using mixtures with point priors are discussed. Computational methods for posterior evaluation and exploration are considered. Rapid updating methods are seen to provide feasible methods for exhaustive evaluation using Gray Code sequencing in moderately sized problems, and fast Markov Chain Monte Carlo exploration in large problems. Estimation of normalization constants is seen to provide improved posterior estimates of individual model probabilities and the total visited probability. Various procedures are illustrated on simulated sample problems and on a real problem concerning the construction of financial index tracking portfolios.},
 author = {Edward I. George and Robert E. McCulloch},
fjournal = {Statistica Sinica},
journal={Stat. Sinica.},
 number = {2},
 pages = {339--373},
 publisher = {Institute of Statistical Science, Academia Sinica},
 title = {Approaches for Bayesian Variable Selection},
 volume = {7},
 year = {1997}
}

@article{brooks-2003,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/3088825},
 abstract = {The major implementational problem for reversible jump Markov chain Monte Carlo methods is that there is commonly no natural way to choose jump proposals since there is no Euclidean structure in the parameter space to guide our choice. We consider mechanisms for guiding the choice of proposal. The first group of methods is based on an analysis of acceptance probabilities for jumps. Essentially, these methods involve a Taylor series expansion of the acceptance probability around certain canonical jumps and turn out to have close connections to Langevin algorithms. The second group of methods generalizes the reversible jump algorithm by using the so-called saturated space approach. These allow the chain to retain some degree of memory so that, when proposing to move from a smaller to a larger model, information is borrowed from the last time that the reverse move was performed. The main motivation for this paper is that, in complex problems, the probability that the Markov chain moves between such spaces may be prohibitively small, as the probability mass can be very thinly spread across the space. Therefore, finding reasonable jump proposals becomes extremely important. We illustrate the procedure by using several examples of reversible jump Markov chain Monte Carlo applications including the analysis of autoregressive time series, graphical Gaussian modelling and mixture modelling.},
 author = {S. P. Brooks and P. Giudici and G. O. Roberts},
JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
 number = {1},
 pages = {3--55},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Efficient Construction of Reversible Jump Markov Chain Monte Carlo Proposal Distributions},
 volume = {65},
 year = {2003}
}

@article{brooks-1998,
 ISSN = {00390526, 14679884},
 URL = {http://www.jstor.org/stable/2988428},
 abstract = {The Markov chain Monte Carlo (MCMC) method, as a computer-intensive statistical tool, has enjoyed an enormous upsurge in interest over the last few years. This paper provides a simple, comprehensive and tutorial review of some of the most common areas of research in this field. We begin by discussing how MCMC algorithms can be constructed from standard building-blocks to produce Markov chains with the desired stationary distribution. We also motivate and discuss more complex ideas that have been proposed in the literature, such as continuous time and dimension jumping methods. We discuss some implementational issues associated with MCMC methods. We take a look at the arguments for and against multiple replications, consider how long chains should be run for and how to determine suitable starting points. We also take a look at graphical models and how graphical approaches can be used to simplify MCMC implementation. Finally, we present a couple of examples, which we use as case-studies to highlight some of the points made earlier in the text. In particular, we use a simple changepoint model to illustrate how to tackle a typical Bayesian modelling problem via the MCMC method, before using mixture model problems to provide illustrations of good sampler output and of the implementation of a reversible jump MCMC algorithm.},
 author = {Stephen P. Brooks},
 journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
 number = {1},
 pages = {69--100},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Markov Chain Monte Carlo Method and Its Application},
 volume = {47},
 year = {1998}
}

@article{malsiner-2016,
 title={Comparing Spike and Slab Priors for Bayesian Variable Selection}, 
 volume={40},
 url={https://www.ajs.or.at/index.php/ajs/article/view/vol40\%2C\%20no4\%20-\%202}, DOI={https://doi.org/10.17713/ajs.v40i4.215}, 
 number={4}, 
 journal={Austrian Journal of Statistics},  
 author={Malsiner-Walli, Gertraud and Wagner, Helga}, 
 year={2016}, 
 month={Feb.},
 pages={241â264}
}

@article{giudici-2003,
author = {Brooks, Stephen and Giudici, Paolo and Philippe, Anne},
year = {2003},
month = {04},
pages = {},
title = {Nonparametric Convergence Assessment for MCMC Model Selection},
volume = {12},
JOURNAL = {J. Comput. Graph. Stat.},
  FJOURNAL = {Journal of Computational and Graphical Statistics},
doi = {10.1198/1061860031347}
}

@article{chambers.1971,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2284221},
 abstract = {An efficient, stable procedure for adding rows to a regression model is given, along with a corresponding procedure for deleting a row. The inherent instability of the latter problem is discussed. The numerical dangers of a commonly used method are pointed out. An example and listings are included.},
 author = {John M. Chambers},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
 number = {336},
 pages = {744--748},
 publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
 title = {Regression Updating},
 volume = {66},
 year = {1971}
}

@inbook{green-2003,
  title     = "Introducing Highly Structured Stochastic Systems",
  author    = "PJ Green",
  year      = "2003",
  language  = "English",
  pages     = "1 -- 12",
  editor    = "PJ Green and NL Hjort and S Richardson",
  booktitle = "Highly Structured Stochastic Systems",
  publisher = "Oxford University Press",
  address   = "United Kingdom",
}

@article {sanger-1977,
	author = {Sanger, F. and Nicklen, S. and Coulson, A. R.},
	title = {DNA sequencing with chain-terminating inhibitors},
	volume = {74},
	number = {12},
	pages = {5463--5467},
	year = {1977},
	doi = {10.1073/pnas.74.12.5463},
	publisher = {National Academy of Sciences},
	abstract = {A new method for determining nucleotide sequences in DNA is described. It is similar to the " plus and minus" method [Sanger, F. \& amp; Coulson, A. R. (1975) J. Mol. Biol. 94, 441-448] but makes use of the 2',3'-dideoxy and arabinonucleoside analogues of the normal deoxynucleoside triphosphates, which act as specific chain-terminating inhibitors of DNA polymerase. The technique has been applied to the DNA of bacteriophage ÏX174 and is more rapid and more accurate than either the plus or the minus method.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/74/12/5463},
	eprint = {https://www.pnas.org/content/74/12/5463.full.pdf},
	fjournal = {Proceedings of the National Academy of Sciences},
journal={Proc. Natl. Acad. Sci. USA.},
}

@article{hastings-1970,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2334940},
 abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
 author = {W. K. Hastings},
 journal = {Biometrika},
 number = {1},
 pages = {97--109},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Monte Carlo Sampling Methods Using Markov Chains and Their Applications},
 volume = {57},
 year = {1970}
}

@book{thisted-1988,
 author = {Thisted, Ronald A.},
 title = {Elements of Statistical Computing: Numerical Computation},
 year = {1988},
 isbn = {0-412-01371-1},
 publisher = {Chapman \& Hall, Ltd.},
 address = {London, UK, UK},
} 



@book{agresti-1990,
	Author = {Agresti, Alan},
	Keywords = {Log-linear model},
	Pages = {572},
	Publisher = {Wiley},
	Title = {Categorical Data Analysis},
	Year = {1990}}

@article{youden,
author = {Stuart G Baker and Barnett S Kramer},
title = {Peirce, Youden, and Receiver Operating Characteristic Curves},
journal = {The American Statistician},
volume = {61},
number = {4},
pages = {343-346},
year  = {2007},
publisher = {Taylor & Francis},
doi = {10.1198/000313007X247643},
URL = { https://doi.org/10.1198/000313007X247643 },
eprint = { https://doi.org/10.1198/000313007X247643}
}

@book {golub_van_loan.2013,
    AUTHOR = {Golub, Gene H. and Van Loan, Charles F.},
     TITLE = {Matrix {C}omputations},
   EDITION = {Fourth},
 PUBLISHER = {Johns Hopkins University Press, Baltimore, MD},
      YEAR = {2013},
     PAGES = {xiv+756},
      ISBN = {978-1-4214-0794-4; 1-4214-0794-9; 978-1-4214-0859-0},
   MRCLASS = {65-02 (65Fxx)},
  MRNUMBER = {3024913},
MRREVIEWER = {J\"{o}rg Liesen},
}

@book {bjorck.2015,
    AUTHOR = {Bj\"{o}rck, {\AA}ke},
     TITLE = {{N}umerical {M}ethods in {M}atrix {C}omputations},
    SERIES = {Texts in Applied Mathematics},
    VOLUME = {59},
 PUBLISHER = {Springer, Cham},
      YEAR = {2015},
     PAGES = {xvi+800},
      ISBN = {978-3-319-05088-1; 978-3-319-05098-8},
   MRCLASS = {65-01 (15-01 65Fxx)},
  MRNUMBER = {3288840},
MRREVIEWER = {Raffaella Pavani},
       DOI = {10.1007/978-3-319-05089-8},
       URL = {https://doi.org/10.1007/978-3-319-05089-8},
}

@article{hastie_etal.2020,
author = "Hastie, Trevor and Tibshirani, Robert and Tibshirani, Ryan",
doi = "10.1214/19-STS733",
fjournal = "Statistical Science",
journal = "Statist. Sci.",
month = "11",
number = "4",
pages = "579--592",
publisher = "The Institute of Mathematical Statistics",
title = "Best subset, forward stepwise or lasso? analysis and recommendations based on extensive comparisons",
url = "https://doi.org/10.1214/19-STS733",
volume = "35",
year = "2020"
}

@article {bhattacharya_dunson.2011,
    AUTHOR = {Bhattacharya, A. and Dunson, D. B.},
     TITLE = {Sparse {B}ayesian infinite factor models},
   JOURNAL = {Biometrika},
  FJOURNAL = {Biometrika},
    VOLUME = {98},
      YEAR = {2011},
    NUMBER = {2},
     PAGES = {291--306},
      ISSN = {0006-3444},
   MRCLASS = {62F15 (62H12 62H25)},
  MRNUMBER = {2806429},
MRREVIEWER = {Nian-Sheng Tang},
       DOI = {10.1093/biomet/asr013},
       URL = {https://doi.org/10.1093/biomet/asr013},
}

@article {dimatteo_etal.2001,
    AUTHOR = {DiMatteo, Ilaria and Genovese, Christopher R. and Kass, Robert
              E.},
     TITLE = {Bayesian curve-fitting with free-knot splines},
   JOURNAL = {Biometrika},
  FJOURNAL = {Biometrika},
    VOLUME = {88},
      YEAR = {2001},
    NUMBER = {4},
     PAGES = {1055--1071},
      ISSN = {0006-3444},
   MRCLASS = {62F15 (62G08)},
  MRNUMBER = {1872219},
       DOI = {10.1093/biomet/88.4.1055},
       URL = {https://doi.org/10.1093/biomet/88.4.1055},
}

@article {lindstrom.2002,
    AUTHOR = {Lindstrom, Mary J.},
     TITLE = {Bayesian estimation of free-knot splines using reversible
              jumps},
   JOURNAL = {Comput. Statist. Data Anal.},
  FJOURNAL = {Computational Statistics \& Data Analysis},
    VOLUME = {41},
      YEAR = {2002},
    NUMBER = {2},
     PAGES = {255--269},
      ISSN = {0167-9473},
   MRCLASS = {62G05 (62F15)},
  MRNUMBER = {1945871},
       DOI = {10.1016/S0167-9473(02)00066-X},
       URL = {https://doi.org/10.1016/S0167-9473(02)00066-X},
}

@article {liang_etal.2001,
    AUTHOR = {Liang, Faming and Truong, Young K. and Wong, Wing Hung},
     TITLE = {Automatic {B}ayesian model averaging for linear regression and
              applications in {B}ayesian curve fitting},
   JOURNAL = {Statist. Sinica},
  FJOURNAL = {Statistica Sinica},
    VOLUME = {11},
      YEAR = {2001},
    NUMBER = {4},
     PAGES = {1005--1029},
      ISSN = {1017-0405},
   MRCLASS = {62F15 (65C05)},
  MRNUMBER = {1867329},
}
		

@article {denison.etal.1998,
    AUTHOR = {Denison, D. G. T. and Mallick, B. K. and Smith, A. F. M.},
     TITLE = {Automatic {B}ayesian curve fitting},
   JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
    VOLUME = {60},
      YEAR = {1998},
    NUMBER = {2},
     PAGES = {333--350},
      ISSN = {1369-7412},
   MRCLASS = {62F15 (62J02)},
  MRNUMBER = {1616029},
       DOI = {10.1111/1467-9868.00128},
       URL = {https://doi.org/10.1111/1467-9868.00128},
}
		
@article{smith_kohn.1996,
title = "Nonparametric regression using Bayesian variable selection",
journal = "Journal of Econometrics",
volume = "75",
number = "2",
pages = "317 - 343",
year = "1996",
issn = "0304-4076",
doi = "https://doi.org/10.1016/0304-4076(95)01763-1",
url = "http://www.sciencedirect.com/science/article/pii/0304407695017631",
author = "Michael Smith and Robert Kohn",
keywords = "Additive model, Power transformation, Gibbs sampler, Regression spline, Robust estimation",
abstract = "This paper estimates an additive model semiparametrically, while automatically selecting the significant independent variables and the appropriate power transformation of the dependent variable. The nonlinear variables are modeled as regression splines, with significant knots selected from a large number of candidate knots. The estimation is made robust by modeling the errors as a mixture of normals. A Bayesian approach is used to select the significant knots, the power transformation, and to identify outliers using the Gibbs sampler to carry out the computation. Empirical evidence is given that the sampler works well on both simulated and real examples and that in the univariate case it compares favorably with a kernel-weighted local linear smoother. The variable selection algorithm in the paper is substantially faster than previous Bayesian variable selection algorithms."
}

@article{kneip_sarda.2011,
author = "Kneip, Alois and Sarda, Pascal",
doi = "10.1214/11-AOS905",
fjournal = "Annals of Statistics",
journal = "Ann. Statist.",
month = "10",
number = "5",
pages = "2410--2447",
publisher = "The Institute of Mathematical Statistics",
title = "Factor models and variable selection in high-dimensional regression analysis",
url = "https://doi.org/10.1214/11-AOS905",
volume = "39",
year = "2011"
}

@article {ormerod_wand.2011,
    AUTHOR = {Wand, M. P. and Ormerod, J. T.},
     TITLE = {Penalized wavelets: embedding wavelets into semiparametric
              regression},
   JOURNAL = {Electron. J. Stat.},
  FJOURNAL = {Electronic Journal of Statistics},
    VOLUME = {5},
      YEAR = {2011},
     PAGES = {1654--1717},
   MRCLASS = {62G08 (42C40 62F15 65T60)},
  MRNUMBER = {2870147},
MRREVIEWER = {Theodore Kypraios},
       DOI = {10.1214/11-EJS652},
       URL = {https://doi.org/10.1214/11-EJS652},
}

@article {hans_etal.2007,
    AUTHOR = {Hans, Chris and Dobra, Adrian and West, Mike},
     TITLE = {Shotgun stochastic search for ``large {$p$}'' regression},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {102},
      YEAR = {2007},
    NUMBER = {478},
     PAGES = {507--516},
      ISSN = {0162-1459},
   MRCLASS = {Expansion},
  MRNUMBER = {2370849},
       DOI = {10.1198/016214507000000121},
       URL = {https://doi.org/10.1198/016214507000000121},
}

@article {liquet_etal.2017,
    AUTHOR = {Liquet, B. and Mengersen, K. and Pettitt, A. N. and Sutton,
              M.},
     TITLE = {Bayesian variable selection regression of multivariate
              responses for group data},
   JOURNAL = {Bayesian Anal.},
  FJOURNAL = {Bayesian Analysis},
    VOLUME = {12},
      YEAR = {2017},
    NUMBER = {4},
     PAGES = {1039--1067},
      ISSN = {1936-0975},
   MRCLASS = {62J05 (62F15 62J07)},
  MRNUMBER = {3724978},
       DOI = {10.1214/17-BA1081},
       URL = {https://doi.org/10.1214/17-BA1081},
}

@article {brown_etal.1998,
    AUTHOR = {Brown, P. J. and Vannucci, M. and Fearn, T.},
     TITLE = {Multivariate {B}ayesian variable selection and prediction},
   JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
    VOLUME = {60},
      YEAR = {1998},
    NUMBER = {3},
     PAGES = {627--641},
      ISSN = {1369-7412},
   MRCLASS = {62F15 (62J05)},
  MRNUMBER = {1626005},
       DOI = {10.1111/1467-9868.00144},
       URL = {https://doi.org/10.1111/1467-9868.00144},
}

@article{forte_etal.2018,
author = {Forte, Anabel and Garcia-Donato, Gonzalo and Steel, Mark},
title = {Methods and Tools for Bayesian Variable Selection and Model Averaging in Normal Linear Regression},
journal = {International Statistical Review},
volume = {86},
number = {2},
pages = {237-258},
doi = {https://doi.org/10.1111/insr.12249},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12249},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12249},
abstract = {Summary In this paper, we briefly review the main methodological aspects concerned with the application of the Bayesian approach to model choice and model averaging in the context of variable selection in regression models. This includes prior elicitation, summaries of the posterior distribution and computational strategies. We then examine and compare various publicly available R-packages, summarizing and explaining the differences between packages and giving recommendations for applied users. We find that all packages reviewed (can) lead to very similar results, but there are potentially important differences in flexibility and efficiency of the packages.},
year = {2018}
}

@article{vehtari.2014,
author = "Vehtari, Aki and Ojanen, Janne",
doi = "10.1214/14-SS105",
fjournal = "Statistics Surveys",
journal = "Statist. Surv.",
pages = "1",
publisher = "The American Statistical Association, the Bernoulli Society, the Institute of Mathematical Statistics, and the Statistical Society of Canada",
title = "Errata: A survey of Bayesian predictive methods for model assessment, selection and comparison",
url = "https://doi.org/10.1214/14-SS105",
volume = "8",
year = "2014"
}

@article{vehtari.2012,
author = "Vehtari, Aki and Ojanen, Janne",
doi = "10.1214/12-SS102",
fjournal = "Statistics Surveys",
journal = "Statist. Surv.",
pages = "142--228",
publisher = "The American Statistical Association, the Bernoulli Society, the Institute of Mathematical Statistics, and the Statistical Society of Canada",
title = "A survey of Bayesian predictive methods for model assessment, selection and comparison",
url = "https://doi.org/10.1214/12-SS102",
volume = "6",
year = "2012"
}

@article{arlot.2010,
author = "Arlot, Sylvain and Celisse, Alain",
doi = "10.1214/09-SS054",
fjournal = "Statistics Surveys",
journal = "Statist. Surv.",
pages = "40--79",
publisher = "The American Statistical Association, the Bernoulli Society, the Institute of Mathematical Statistics, and the Statistical Society of Canada",
title = "A survey of cross-validation procedures for model selection",
url = "https://doi.org/10.1214/09-SS054",
volume = "4",
year = "2010"
}

@article{george_mcculloch.1993,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2290777},
 abstract = {A crucial problem in building a multiple regression model is the selection of predictors to include. The main thrust of this article is to propose and develop a procedure that uses probabilistic considerations for selecting promising subsets. This procedure entails embedding the regression setup in a hierarchical normal mixture model where latent variables are used to identify subset choices. In this framework the promising subsets of predictors can be identified as those with higher posterior probability. The computational burden is then alleviated by using the Gibbs sampler to indirectly sample from this multinomial posterior distribution on the set of possible subset choices. Those subsets with higher probability--the promising ones--can then be identified by their more frequent appearance in the Gibbs sample.},
 author = {Edward I. George and Robert E. McCulloch},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
 number = {423},
 pages = {881-889},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Variable selection via gibbs sampling},
 volume = {88},
 year = {1993}
}

@article{rockova_george.2014,
author = {Veronika Ro\v{c}kov\'{a} and Edward I. George},
title = {EMVS: The EM Approach to Bayesian Variable Selection},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
volume = {109},
number = {506},
pages = {828-846},
year = {2014},
doi = {10.1080/01621459.2013.869223},
URL = {http://dx.doi.org/10.1080/01621459.2013.869223},
eprint = {http://dx.doi.org/10.1080/01621459.2013.869223}
}

@article{george_mcculloch.1997,
 ISSN = {10170405, 19968507},
 URL = {http://www.jstor.org/stable/24306083},
 author = {Edward I. George and Robert E. McCulloch},
fjournal = {Statistica Sinica},
journal={Stat. Sinica.},
 number = {2},
 pages = {339--373},
 publisher = {Institute of Statistical Science, Academia Sinica},
 title = {Approaches for Bayesian variable selection},
 volume = {7},
 year = {1997}
}

@article{mitchell_beauchamp.1988,
author = { T. J.   Mitchell  and  J. J.   Beauchamp },
title = {Bayesian variable selection in linear regression},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
volume = {83},
number = {404},
pages = {1023-1032},
year = {1988},
doi = {10.1080/01621459.1988.10478694},
URL = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478694},
eprint = {http://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478694}
}

@article {bottolo_richardson.2010,
    AUTHOR = {Bottolo, Leonard and Richardson, Sylvia},
     TITLE = {Evolutionary stochastic search for {B}ayesian model
              exploration},
   JOURNAL = {Bayesian Anal.},
  FJOURNAL = {Bayesian Analysis},
    VOLUME = {5},
      YEAR = {2010},
    NUMBER = {3},
     PAGES = {583--618},
      ISSN = {1936-0975},
   MRCLASS = {Expansion},
  MRNUMBER = {2719668},
       DOI = {10.1214/10-BA523},
       URL = {https://doi.org/10.1214/10-BA523},
}

@article{bottolo_etal.2011,
    author = {Bottolo, Leonardo and Chadeau-Hyam, Marc and Hastie, David I. and Langley, Sarah R. and Petretto, Enrico and Tiret, Laurence and Tregouet, David and Richardson, Sylvia},
    title = "{ESS++: a C++ objected-oriented algorithm for Bayesian stochastic search model exploration}",
    journal = {Bioinformatics},
    volume = {27},
    number = {4},
    pages = {587-588},
    year = {2011},
    month = {01},
    abstract = "{Summary:ESS++ is a C++ implementation of a fully Bayesian variable selection approach for single and multiple response linear regression. ESS++ works well both when the number of observations is larger than the number of predictors and in the âlarge p, small nâ case. In the current version, ESS++ can handle several hundred observations, thousands of predictors and a few responses simultaneously. The core engine of ESS++ for the selection of relevant predictors is based on Evolutionary Monte Carlo. Our implementation is open source, allowing community-based alterations and improvements.Availability: C++ source code and documentation including compilation instructions are available under GNU licence at http://bgx.org.uk/software/ESS.html.Contact:l.bottolo@imperial.ac.ukSupplementary information:Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btq684},
    url = {https://doi.org/10.1093/bioinformatics/btq684},
    eprint = {http://oup.prod.sis.lan/bioinformatics/article-pdf/27/4/587/16902135/btq684.pdf},
}

@article{petralias_dellaportas.2013,
author = { Athanassios   Petralias  and  Petros   Dellaportas },
title = {An MCMC model search algorithm for regression problems},
journal = {Journal of Statistical Computation and Simulation},
volume = {83},
number = {9},
pages = {1722-1740},
year  = {2013},
publisher = {Taylor & Francis},
doi = {10.1080/00949655.2012.668907},
URL = {https://doi.org/10.1080/00949655.2012.668907},
eprint = {https://doi.org/10.1080/00949655.2012.668907}
}

@article{papathomas_etal.2011,
    author = {Papathomas, M. and Dellaportas, P. and Vasdekis, V. G. S.},
    title = "{A novel reversible jump algorithm for generalized linear models}",
    journal = {Biometrika},
    volume = {98},
    number = {1},
    pages = {231-236},
    year = {2011},
    month = {02},
    abstract = "{We propose a novel methodology to construct proposal densities in reversible jump algorithms that obtain samples from parameter subspaces of competing generalized linear models with differing dimensions. The derived proposal densities are not restricted to moves between nested models and are applicable even to models that share no common parameters. We illustrate our methodology on competing logistic regression and log-linear graphical models, demonstrating how our suggested proposal densities, together with the resulting freedom to propose moves between any models, improve the performance of the reversible jump algorithm.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asq071},
    url = {https://doi.org/10.1093/biomet/asq071},
    eprint = {http://oup.prod.sis.lan/biomet/article-pdf/98/1/231/592616/asq071.pdf},
}

@article {clyde_george.2004,
    AUTHOR = {Clyde, Merlise and George, Edward I.},
     TITLE = {Model uncertainty},
   JOURNAL = {Statist. Sci.},
  FJOURNAL = {Statistical Science. A Review Journal of the Institute of
              Mathematical Statistics},
    VOLUME = {19},
      YEAR = {2004},
    NUMBER = {1},
     PAGES = {81--94},
      ISSN = {0883-4237},
   MRCLASS = {62C10},
  MRNUMBER = {2082148},
       DOI = {10.1214/088342304000000035},
       URL = {https://doi.org/10.1214/088342304000000035},
}

@incollection {chipman_etal.2001,
    AUTHOR = {Chipman, Hugh and George, Edward I. and McCulloch, Robert E.},
     TITLE = {The practical implementation of {B}ayesian model selection},
 BOOKTITLE = {Model selection},
    SERIES = {IMS Lecture Notes Monogr. Ser.},
    VOLUME = {38},
     PAGES = {65--134},
 PUBLISHER = {Inst. Math. Statist., Beachwood, OH},
      YEAR = {2001},
   MRCLASS = {62C10 (62C12)},
  MRNUMBER = {2000752},
       DOI = {10.1214/lnms/1215540964},
       URL = {https://doi.org/10.1214/lnms/1215540964},
}

@article{clyde_etal.1996,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2291738},
 abstract = {We introduce an approach and algorithms for model mixing in large prediction problems with correlated predictors. We focus on the choice of predictors in linear models, and mix over possible subsets of candidate predictors. Our approach is based on expressing the space of models in terms of an orthogonalization of the design matrix. Advantages are both statistical and computational. Statistically, orthogonalization often leads to a reduction in the number of competing models by eliminating correlations. Computationally, large model spaces cannot be enumerated; recent approaches are based on sampling models with high posterior probability via Markov chains. Based on orthogonalization of the space of candidate predictors, we can approximate the posterior probabilities of models by products of predictor-specific terms. This leads to an importance sampling function for sampling directly from the joint distribution over the model space, without resorting to Markov chains. Compared to the latter, orthogonalized model mixing by importance sampling is faster in sampling models and is also more efficient in finding models that contribute significantly to the prediction. Further advantages are in the speed of convergence and the availability of more reliable convergence diagnostic tools. We illustrate these in practice, using a data set on prediction of crime rates. The model space is small enough so that enumeration of all models is available for comparison and convergence checks. Also, we demonstrate the feasibility of orthogonalized model mixing in a large-size problem, which is very difficult to attack by other methods. The data set is from a designed experiment dealing with predicting protein activity under different storage conditions. The model space is large (the rank of the design matrix is 88) and very difficult to explore if expressed in terms of the original variables. We obtain prediction intervals and a probability distribution of the setting that produces the highest response.},
 author = {Merlise Clyde and Heather Desimone and Giovanni Parmigiani},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
 number = {435},
 pages = {1197--1208},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Prediction Via Orthogonalized Model Mixing},
 volume = {91},
 year = {1996}
}



@ARTICLE{green-1995,
    author = {Peter J. Green},
    title = {Reversible jump Markov chain Monte Carlo computation and Bayesian model determination},
    journal = {Biometrika},
    year = {1995},
    volume = {82},
    pages = {711--732}
}

@article {rockova_george.2018,
    AUTHOR = {Ro\v{c}kov\'{a}, Veronika and George, Edward I.},
     TITLE = {The spike-and-slab {LASSO}},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {113},
      YEAR = {2018},
    NUMBER = {521},
     PAGES = {431--444},
      ISSN = {0162-1459},
   MRCLASS = {62J07 (62F15)},
  MRNUMBER = {3803476},
       DOI = {10.1080/01621459.2016.1260469},
       URL = {https://doi.org/10.1080/01621459.2016.1260469},
}

@article {leamer.1978,
    AUTHOR = {Leamer, Edward E.},
     TITLE = {Regression selection strategies and revealed priors},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {73},
      YEAR = {1978},
    NUMBER = {363},
     PAGES = {580--587},
      ISSN = {0003-1291},
   MRCLASS = {62J02 (62F15)},
  MRNUMBER = {514159},
MRREVIEWER = {Lyle D. Broemeling},
       URL =
              {http://links.jstor.org/sici?sici=0162-1459(197809)73:363<580:RSSARP>2.0.CO;2-U&origin=MSN},
}

@incollection {geweke.1996,
    AUTHOR = {Geweke, J.},
     TITLE = {Variable selection and model comparison in regression},
 BOOKTITLE = {Bayesian statistics, 5 ({A}licante, 1994)},
    SERIES = {Oxford Sci. Publ.},
     PAGES = {609--620},
 PUBLISHER = {Oxford Univ. Press, New York},
      YEAR = {1996},
   MRCLASS = {62J05},
  MRNUMBER = {1425430},
}

@article{carlin_chib.1995,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346151},
 abstract = {Markov chain Monte Carlo (MCMC) integration methods enable the fitting of models of virtually unlimited complexity, and as such have revolutionized the practice of Bayesian data analysis. However, comparison across models may not proceed in a completely analogous fashion, owing to violations of the conditions sufficient to ensure convergence of the Markov chain. In this paper we present a framework for Bayesian model choice, along with an MCMC algorithm that does not suffer from convergence difficulties. Our algorithm applies equally well to problems where only one model is contemplated but its proper size is not known at the outset, such as problems involving integer-valued parameters, multiple changepoints or finite mixture distributions. We illustrate our approach with two published examples.},
 author = {Bradley P. Carlin and Siddhartha Chib},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {3},
 pages = {473--484},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Bayesian Model Choice via Markov Chain Monte Carlo Methods},
 volume = {57},
 year = {1995}
}

@article{dellaportas.2002,
author="Dellaportas, Petros
and Forster, Jonathan J.
and Ntzoufras, Ioannis",
title="On Bayesian model and variable selection using MCMC",
journal="Statistics and Computing",
year="2002",
month="Jan",
day="01",
volume="12",
number="1",
pages="27--36",
abstract="Several MCMC methods have been proposed for estimating probabilities of models and associated 'model-averaged' posterior distributions in the presence of model uncertainty. We discuss, compare, develop and illustrate several of these methods, focussing on connections between them.",
issn="1573-1375",
doi="10.1023/A:1013164120801",
url="https://doi.org/10.1023/A:1013164120801"
}

@article {faming_etal.2001,
    AUTHOR = {Liang, Faming and Truong, Young K. and Wong, Wing Hung},
     TITLE = {Automatic {B}ayesian model averaging for linear regression and
              applications in {B}ayesian curve fitting},
   JOURNAL = {Statist. Sinica},
  FJOURNAL = {Statistica Sinica},
    VOLUME = {11},
      YEAR = {2001},
    NUMBER = {4},
     PAGES = {1005--1029},
      ISSN = {1017-0405},
   MRCLASS = {62F15 (65C05)},
  MRNUMBER = {1867329},
}

@article {raftery_etal.1997,
    AUTHOR = {Raftery, Adrian E. and Madigan, David and Hoeting, Jennifer
              A.},
     TITLE = {Bayesian model averaging for linear regression models},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {92},
      YEAR = {1997},
    NUMBER = {437},
     PAGES = {179--191},
      ISSN = {0162-1459},
   MRCLASS = {62J05},
  MRNUMBER = {1436107},
       DOI = {10.2307/2291462},
       URL = {https://doi.org/10.2307/2291462},
}

@article{brown_etal.1998b,
author = {Brown, P. J. and Vannucci, M. and Fearn, T.},
title = {Bayesian wavelength selection in multicomponent analysis},
journal = {Journal of Chemometrics},
volume = {12},
number = {3},
pages = {173-182},
keywords = {multivariate regression, Bayesian wavelength selection, Markov chain Monte Carlo (MCMC), Metropolis algorithm, NIR spectroscopy, multicomponent analysis, selection bias, model averaging},
doi = {10.1002/(SICI)1099-128X(199805/06)12:3<173::AID-CEM505>3.0.CO;2-0},
url = {https://onlinelibrary.wiley.com/doi/abs10.1002/%28SICI%291099-128X%28199805/06%2912%3A3%3C173%3A%3AAID-CEM505%3E3.0.CO%3B2-0},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291099-128X%28199805/06%2912%3A3%3C173%3A%3AAID-CEM505%3E3.0.CO%3B2-0},
abstract = {Abstract Multicomponent analysis attempts to simultaneously predict the ingredients of a mixture. If near-infrared spectroscopy provides the predictor variables, then modern scanning instruments may offer absorbances at a very large number of wavelengths. Although it is perfectly possible to use whole spectrum methods (e.g. PLS, ridge and principal component regression), for a number of reasons it is often desirable to select a small number of wavelengths from which to construct the prediction equation relating absorbances to composition. This paper considers wavelength selection with a view to using the chosen wavelengths to simultaneously predict the compositional ingredients and is therefore an example of multivariate variable selection. It adopts a binary exclusion/inclusion latent variable formulation of selection and uses a Bayesian approach. Problems of search of the vast number of possible selected models are overcome by a Markov chain Monte Carlo sampling technique. Â© 1998 John Wiley \& Sons, Ltd.},
year = {1998}
}

@article{ishwaran_rao.2005,
author = "Ishwaran, Hemant and Rao, J. Sunil",
doi = "10.1214/009053604000001147",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "04",
number = "2",
pages = "730--773",
publisher = "The Institute of Mathematical Statistics",
title = "Spike and slab variable selection: Frequentist and Bayesian strategies",
url = "http://dx.doi.org/10.1214/009053604000001147",
volume = "33",
year = "2005"
}

@article{ohara_sillanpaa.2009,
author = "O'Hara, R. B. and Sillanp{\"a}{\"a}, M. J.",
doi = "10.1214/09-BA403",
fjournal = "Bayesian Analysis",
journal = "Bayesian Anal.",
month = "03",
number = "1",
pages = "85--117",
publisher = "International Society for Bayesian Analysis",
title = "A review of Bayesian variable selection methods: what, how and which",
url = "http://dx.doi.org/10.1214/09-BA403",
volume = "4",
year = "2009"
}

@article {cui_george.2008,
    AUTHOR = {Cui, Wen and George, Edward I.},
     TITLE = {Empirical {B}ayes vs. fully {B}ayes variable selection},
   JOURNAL = {J. Statist. Plann. Inference},
  FJOURNAL = {Journal of Statistical Planning and Inference},
    VOLUME = {138},
      YEAR = {2008},
    NUMBER = {4},
     PAGES = {888--900},
      ISSN = {0378-3758},
   MRCLASS = {Expansion},
  MRNUMBER = {2416869},
       DOI = {10.1016/j.jspi.2007.02.011},
       URL = {https://doi.org/10.1016/j.jspi.2007.02.011}
}

@article {liang_etal.2008,
    AUTHOR = {Liang, Feng and Paulo, Rui and Molina, German and Clyde,
              Merlise A. and Berger, Jim O.},
     TITLE = {Mixtures of {$g$} priors for {B}ayesian variable selection},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {103},
      YEAR = {2008},
    NUMBER = {481},
     PAGES = {410--423},
      ISSN = {0162-1459},
   MRCLASS = {Expansion},
  MRNUMBER = {2420243},
       DOI = {10.1198/016214507000001337},
       URL = {https://doi.org/10.1198/016214507000001337}
}
	
@article {maruyama_george.2011,
    AUTHOR = {Maruyama, Yuzo and George, Edward I.},
     TITLE = {Fully {B}ayes factors with a generalized {$g$}-prior},
   JOURNAL = {Ann. Statist.},
  FJOURNAL = {The Annals of Statistics},
    VOLUME = {39},
      YEAR = {2011},
    NUMBER = {5},
     PAGES = {2740--2765},
      ISSN = {0090-5364},
   MRCLASS = {62F07 (62C10 62F15 62J07)},
  MRNUMBER = {2906885},
MRREVIEWER = {Marvin H. J. Gruber},
       DOI = {10.1214/11-AOS917},
       URL = {https://doi.org/10.1214/11-AOS917}
}	

@article {scott_berger.2010,
    AUTHOR = {Scott, James G. and Berger, James O.},
     TITLE = {Bayes and empirical-{B}ayes multiplicity adjustment in the
              variable-selection problem},
   JOURNAL = {Ann. Statist.},
  FJOURNAL = {The Annals of Statistics},
    VOLUME = {38},
      YEAR = {2010},
    NUMBER = {5},
     PAGES = {2587--2619},
      ISSN = {0090-5364},
   MRCLASS = {62J05 (62C12 62F15 62J15)},
  MRNUMBER = {2722450},
MRREVIEWER = {Marvin H. J. Gruber},
       DOI = {10.1214/10-AOS792},
       URL = {https://doi.org/10.1214/10-AOS792},
}

@article{green.1995,
     jstor_articletype = {research-article},
     title = {Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination},
     author = {Green, Peter J.},
     journal = {Biometrika},
     jstor_issuetitle = {},
     volume = {82},
     number = {4},
     jstor_formatteddate = {Dec., 1995},
     pages = {pp. 711-732},
     url = {http://www.jstor.org/stable/2337340},
     ISSN = {00063444},
     language = {English},
     year = {1995},
     publisher = {Biometrika Trust},
     copyright = {Copyright ï¿œ 1995 Biometrika Trust},
}

@article{hastie_green.2012,
author = {Hastie, David I. and Green, Peter J.},
title = {Model choice using reversible jump Markov chain Monte Carlo},
journal = {Statistica Neerlandica},
volume = {66},
number = {3},
pages = {309-338},
keywords = {across-model sampling, Bayes factors, Bayesian model determination, posterior model probabilities, transdimensional inference, variable dimension problems},
doi = {10.1111/j.1467-9574.2012.00516.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9574.2012.00516.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9574.2012.00516.x},
abstract = {We review the across-model simulation approach to computation for Bayesian model determination, based on the reversible jump Markov chain Monte Carlo method. Advantages, difficulties and variations of the methods are discussed. We also discuss some limitations of the ideal Bayesian view of the model determination problem, for which no computational methods can provide a cure.},
year = {2012}
}

@article{madigan_etal.1995,
 ISSN = {03067734, 17515823},
 URL = {http://www.jstor.org/stable/1403615},
 abstract = {For more than half a century, data analysts have used graphs to represent statistical models. In particular, graphical "conditional independence" models have emerged as a useful class of models. Applications of such models to probabilistic expert systems, image analysis, and pedigree analysis have motivated much of this work, and several expository texts are now available. Rather less well known is the development of a Bayesian framework for such models. Expert system applications have motivated this work, where the promise of a model that can update itself as data become available, has generated intense interest from the artificial intelligence community. However, the application to a broader range of data problems has been largely overlooked. The purpose of this article is to show how Bayesian graphical models unify and simplify many standard discrete data problems such as Bayesian log linear modeling with either complete or incomplete data, closed population estimation, and double sampling. Since conventional model selection fails in these applications, we construct posterior distributions for quantities of interest by averaging across models. Specifically we introduce Markov chain Monte Carlo model composition, a Monte Carlo method for Bayesian model averaging. /// Pendant plus d'un demi-siècle, les graphes ont été utilisés pour représenter des modéles statistiques en analyse de donneés. En particulier, les graphes décrivant l'indépendance conditionnelle sont apparus comme une classe importante de ces modèles. Des applications en analyse d'image, analyse de pedigree, ou encore en système expert sont à l'origine de leur développement, et plusieurs livres de synthèse ont déjà été publiés à ce sujet. Le développement d'un cadre Bayésien de ces modèles est en revanche moins connu, et les applications en systèmes experts ont motivé la recherche dans ce domaine. La possibilité de construire des modèles capables de se remettre à jour au fur et à mesure que de nouvelles données sont disponibles est à l'origine d'un intérêt intense de la part de la communauté travaillant en intelligence artificielle. Cependent, leur application a une classe plus vaste d'analyse de données a été largement négligée. L'objet de cet article est de montrer comment les modèles Bayésiens de graphes permettent d'unifier et de simplifier des problèmes standards, tels que les modèles log-linéaires Bayésiens (avec des données complètes ou non), l'estimation d'une population fermée ou le double échantillonnage. Dans la mesure ou le choix d'un modèle conventionel unique échoue dans ce type de situation, nous construisons des distributions a posteriori des quantités d'intérêt en moyennant sur les modèles possibles. Plus particulièrement, nous introduisons la composition de chaînes de Markov-Monte Carlo, une méthode de Monte-Carlo permettant de moyenner sur les modèles retenus.},
 author = {David Madigan and Jeremy York and Denis Allard},
 journal = {International Statistical Review / Revue Internationale de Statistique},
 number = {2},
 pages = {215--232},
 publisher = {[Wiley, International Statistical Institute (ISI)]},
 title = {Bayesian Graphical Models for Discrete Data},
 volume = {63},
 year = {1995}
}

@article {brown_etal.2002,
    AUTHOR = {Brown, P. J. and Vannucci, M. and Fearn, T.},
     TITLE = {Bayes model averaging with selection of regressors},
   JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
    VOLUME = {64},
      YEAR = {2002},
    NUMBER = {3},
     PAGES = {519--536},
      ISSN = {1369-7412},
   MRCLASS = {62J05 (62F15)},
  MRNUMBER = {1924304},
       DOI = {10.1111/1467-9868.00348},
       URL = {https://doi.org/10.1111/1467-9868.00348},
}



@article{hans.2009b,
author="Hans, Chris",
title="Model uncertainty and variable selection inÂ Bayesian lasso regression",
journal="Statistics and Computing",
year="2009",
volume="20",
number="2",
pages="221--229",
abstract="While Bayesian analogues of lasso regression have become popular, comparatively little has been said about formal treatments of model uncertainty in such settings. This paper describes methods that can be used to evaluate the posterior distribution over the space of all possible regression models for Bayesian lasso regression. Access to the model space posterior distribution is necessary if model-averaged inference---e.g., model-averaged prediction and calculation of posterior variable inclusion probabilities---is desired. The key element of all such inference is the ability to evaluate the marginal likelihood of the data under a given regression model, which has so far proved difficult for the Bayesian lasso. This paper describes how the marginal likelihood can be accurately computed when the number of predictors in the model is not too large, allowing for model space enumeration when the total number of possible predictors is modest. In cases where the total number of possible predictors is large, a simple Markov chain Monte Carlo approach for sampling the model space posterior is provided. This Gibbs sampling approach is similar in spirit to the stochastic search variable selection methods that have become one of the main tools for addressing Bayesian regression model uncertainty, and the adaption of these methods to the Bayesian lasso is shown to be straightforward.",
issn="1573-1375",
doi="10.1007/s11222-009-9160-9",
url="http://dx.doi.org/10.1007/s11222-009-9160-9"
}

@article{hans.2011,
author = {Chris Hans},
title = {Elastic Net Regression Modeling With the Orthant Normal Prior},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
volume = {106},
number = {496},
pages = {1383-1393},
year = {2011},
doi = {10.1198/jasa.2011.tm09241},
URL = {http://dx.doi.org/10.1198/jasa.2011.tm09241},
eprint = {http://dx.doi.org/10.1198/jasa.2011.tm09241}
}

@article{li_lin.2010,
author = "Li, Qing and Lin, Nan",
doi = "10.1214/10-BA506",
fjournal = "Bayesian Analysis",
journal = "Bayesian Anal.",
month = "03",
number = "1",
pages = "151--170",
publisher = "International Society for Bayesian Analysis",
title = "The Bayesian elastic net",
url = "https://doi.org/10.1214/10-BA506",
volume = "5",
year = "2010"
}

@book {robert_casella.2004,
    AUTHOR = {Robert, Christian P. and Casella, George},
     TITLE = {Monte {C}arlo statistical methods},
   EDITION = {Second},
 PUBLISHER = {Springer-Verlag, New York},
      YEAR = {2004},
     PAGES = {xxx+645},
      ISBN = {0-387-21239-6},
   MRCLASS = {62-01 (60J10 62F15 65Cxx)},
  MRNUMBER = {2080278},
MRREVIEWER = {Petru P. Blaga},
       DOI = {10.1007/978-1-4757-4145-2},
       URL = {https://doi.org/10.1007/978-1-4757-4145-2},
}

@inbook{robert.2016,
author = {Robert, Christian P.},
publisher = {John Wiley \& Sons, Ltd},
isbn = {9781118445112},
title = {The {M}etropolis–{H}astings {A}lgorithm},
booktitle = {Wiley StatsRef: Statistics Reference Online},
chapter = {},
pages = {1-15},
doi = {https://doi.org/10.1002/9781118445112.stat07834},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat07834},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat07834},
year = {2015},
keywords = {               Bayesian inference,                Markov chains,                MCMC methods,                Metropolis–Hastings algorithm, intractable density,                Gibbs sampler,                Langevin diffusion,                Hamiltonian Monte Carlo},
abstract = {Abstract This article is a self-contained introduction to the Metropolis–Hastings algorithm, the ubiquitous tool for producing dependent simulations from an arbitrary distribution. The document illustrates the principles of the methodology on simple examples with R codes and provides entries to the recent extensions of the method.}
}

@InProceedings{cornish_etal.2019,
  title = 	 {Scalable {M}etropolis-{H}astings for Exact {B}ayesian Inference with Large Datasets},
  author =       {Cornish, Rob and Vanetti, Paul and Bouchard-Cote, Alexandre and Deligiannidis, George and Doucet, Arnaud},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1351--1360},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/cornish19a/cornish19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/cornish19a.html},
  abstract = 	 {Bayesian inference via standard Markov Chain Monte Carlo (MCMC) methods such as Metropolis-Hastings is too computationally intensive to handle large datasets, since the cost per step usually scales like $O(n)$ in the number of data points $n$. We propose the Scalable Metropolis-Hastings (SMH) kernel that only requires processing on average $O(1)$ or even $O(1/\sqrt{n})$ data points per step. This scheme is based on a combination of factorized acceptance probabilities, procedures for fast simulation of Bernoulli processes, and control variate ideas. Contrary to many MCMC subsampling schemes such as fixed step-size Stochastic Gradient Langevin Dynamics, our approach is exact insofar as the invariant distribution is the true posterior and not an approximation to it. We characterise the performance of our algorithm theoretically, and give realistic and verifiable conditions under which it is geometrically ergodic. This theory is borne out by empirical results that demonstrate overall performance benefits over standard Metropolis-Hastings and various subsampling algorithms.}
}

@article {richardson_green.1997,
    AUTHOR = {Richardson, Sylvia and Green, Peter J.},
     TITLE = {On {B}ayesian analysis of mixtures with an unknown number of
              components},
   JOURNAL = {J. Roy. Statist. Soc. Ser. B},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Methodological},
    VOLUME = {59},
      YEAR = {1997},
    NUMBER = {4},
     PAGES = {731--792},
      ISSN = {0035-9246},
   MRCLASS = {62F15},
  MRNUMBER = {1483213},
       DOI = {10.1111/1467-9868.00095},
       URL = {https://doi.org/10.1111/1467-9868.00095},
}

@article {kuo_etal.1998,
    AUTHOR = {Kuo, Lynn and Mallick, Bani},
     TITLE = {Variable selection for regression models},
   JOURNAL = {Sankhy\={a} Ser. B},
  FJOURNAL = {Sankhy\={a}. The Indian Journal of Statistics. Series B},
    VOLUME = {60},
      YEAR = {1998},
    NUMBER = {1},
     PAGES = {65--81},
      ISSN = {0581-5738},
   MRCLASS = {62J12 (62F15)},
  MRNUMBER = {1717076},
}

@article{mallick.1998,
title = "Bayesian curve estimation by polynomial of random order",
JOURNAL = {J. Statist. Plann. Inference},
  FJOURNAL = {Journal of Statistical Planning and Inference},
volume = "70",
number = "1",
pages = "91 - 109",
year = "1998",
issn = "0378-3758",
doi = "https://doi.org/10.1016/S0378-3758(97)00179-1",
url = "http://www.sciencedirect.com/science/article/pii/S0378375897001791",
author = "Bani.K. Mallick",
keywords = "Bayesian nonparametric regression, Polynomial regression, Random order polynomials, Discontinuities, Piecewise polynomials, Reversible jump MCMC",
abstract = "A Bayesian method of estimating an unknown regression curve by a polynomial of random order is proposed. A joint distribution is assigned over both the set of possible orders of the polynomial and the polynomial coefficients. Reversible jumps Markov chain Monte Carlo (MCMC) (Green, Biometrika 82 (1995) 711-32), are used to compute required posteriors. The methodology is extended to polynomials of random order with discontinuities and to piecewise polynomials of random order to handle wiggly curves. The effectiveness of the methodology is illustrated with a number of examples."
}

@article{heinze_etal.2018,
author = {Heinze, Georg and Wallisch, Christine and Dunkler, Daniela},
title = {Variable selection - A review and recommendations for the practicing statistician},
journal = {Biometrical Journal},
volume = {60},
number = {3},
pages = {431-449},
keywords = {change-in-estimate criterion, penalized likelihood, resampling, statistical model, stepwise selection},
doi = {https://doi.org/10.1002/bimj.201700067},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.201700067},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201700067},
abstract = {Abstract Statistical models support medical research by facilitating individualized outcome prognostication conditional on independent variables or by estimating effects of risk factors adjusted for covariates. Theory of statistical models is well-established if the set of independent variables to consider is fixed and small. Hence, we can assume that effect estimates are unbiased and the usual methods for confidence interval estimation are valid. In routine work, however, it is not known a priori which covariates should be included in a model, and often we are confronted with the number of candidate variables in the range 10â30. This number is often too large to be considered in a statistical model. We provide an overview of various available variable selection methods that are based on significance or information criteria, penalized likelihood, the change-in-estimate criterion, background knowledge, or combinations thereof. These methods were usually developed in the context of a linear regression model and then transferred to more generalized linear models or models for censored survival data. Variable selection, in particular if used in explanatory modeling where effect estimates are of central interest, can compromise stability of a final model, unbiasedness of regression coefficients, and validity of p-values or confidence intervals. Therefore, we give pragmatic recommendations for the practicing statistician on application of variable selection methods in general (low-dimensional) modeling problems and on performing stability investigations and inference. We also propose some quantities based on resampling the entire variable selection process to be routinely reported by software packages offering automated variable selection algorithms.},
year = {2018}
}

@article {zellner.1962,
    AUTHOR = {Zellner, Arnold},
     TITLE = {An efficient method of estimating seemingly unrelated
              regressions and tests for aggregation bias},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {57},
      YEAR = {1962},
     PAGES = {348--368},
      ISSN = {0162-1459},
   MRCLASS = {62.55},
  MRNUMBER = {0139235},
MRREVIEWER = {P. G. Guest},
       URL =
              {http://links.jstor.org/sici?sici=0162-1459(196206)57:298<348:AEMOES>2.0.CO;2-6&origin=MSN},
}
				
		
@article {zellner.1963,
    AUTHOR = {Zellner, Arnold},
     TITLE = {Estimators for seemingly unrelated equations: {S}ome exact
              finite sample results},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {58},
      YEAR = {1963},
     PAGES = {977--992},
      ISSN = {0162-1459},
   MRCLASS = {62.55 (62.99)},
  MRNUMBER = {0157439},
MRREVIEWER = {P. G. Guest},
       URL =
              {http://links.jstor.org/sici?sici=0162-1459(196312)58:304<977:EFSURE>2.0.CO;2-R&origin=MSN},
}		

@book {lutkepohl.2005,
    AUTHOR = {L\"utkepohl, Helmut},
     TITLE = {New introduction to multiple time series analysis},
 PUBLISHER = {Springer-Verlag, Berlin},
      YEAR = {2005},
     PAGES = {xxii+764},
      ISBN = {3-540-40172-5},
   MRCLASS = {62-01 (62M10)},
  MRNUMBER = {2172368},
MRREVIEWER = {Ra\'ul Pedro Mentz},
       URL = {https://doi.org/10.1007/978-3-540-27752-1},
}

@incollection {richardson_etal.2011,
    AUTHOR = {Richardson, Sylvia and Bottolo, Leonardo and Rosenthal,
              Jeffrey S.},
     TITLE = {Bayesian models for sparse regression analysis of high
              dimensional data},
 BOOKTITLE = {Bayesian statistics 9},
     PAGES = {539--568},
      NOTE = {With discussions by Bani Mallick, Soma S. Dhavala, Faming
              Liang, Rajesh Talluri and Mingqi Wu},
 PUBLISHER = {Oxford Univ. Press, Oxford},
      YEAR = {2011},
   MRCLASS = {62F15 (62F07 62J05 62P10)},
  MRNUMBER = {3204018},
       DOI = {10.1093/acprof:oso/9780199694587.003.0018},
       URL = {https://doi.org/10.1093/acprof:oso/9780199694587.003.0018},
}
		
		
@article{bottolo_etal.2020,
    author = {Bottolo, Leonardo and Banterle, Marco and Richardson, Sylvia and Ala-Korpela, Mika and J{\"a}rvelin, Marjo-Riitta and Lewin, Alex},
    title = {A computationally efficient Bayesian seemingly unrelated regressions model for high-dimensional quantitative trait loci discovery},
    fjournal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
    JOURNAL = {J. R. Stat. Soc. Ser. C Appl. Stat.},
    volume = {70},
    number = {4},
    pages = {886-908},
    year = {2021},
    month = {08},
    abstract = {Our work is motivated by the search for metabolite quantitative trait loci (QTL) in a cohort of more than 5000 people. There are 158 metabolites measured by NMR spectroscopy in the 31-year follow-up of the Northern Finland Birth Cohort 1966 (NFBC66). These metabolites, as with many multivariate phenotypes produced by high-throughput biomarker technology, exhibit strong correlation structures. Existing approaches for combining such data with genetic variants for multivariate QTL analysis generally ignore phenotypic correlations or make restrictive assumptions about the associations between phenotypes and genetic loci. We present a computationally efficient Bayesian seemingly unrelated regressions model for high-dimensional data, with cell-sparse variable selection and sparse graphical structure for covariance selection. Cell sparsity allows different phenotype responses to be associated with different genetic predictors and the graphical structure is used to represent the conditional dependencies between phenotype variables. To achieve feasible computation of the large model space, we exploit a factorisation of the covariance matrix. Applying the model to the NFBC66 data with 9000 directly genotyped single nucleotide polymorphisms, we are able to simultaneously estimate genotype–phenotype associations and the residual dependence structure among the metabolites. The R package BayesSUR with full documentation is available at https://cran.r-project.org/web/packages/BayesSUR/},
    issn = {0035-9254},
    doi = {10.1111/rssc.12490},
    url = {https://doi.org/10.1111/rssc.12490},
    eprint = {https://academic.oup.com/jrsssc/article-pdf/70/4/886/49167034/rssc\_70\_4\_886.pdf},
}

@article{scott_etal.1985,
 ISSN = {00031305},
 URL = {http://www.jstor.org/stable/2682815},
 abstract = {Procedures are presented for reducing a data matrix to triangular form by using orthogonal transformations. It is shown how an analysis of variance can be constructed from the triangular reduction of the data matrix. Procedures for calculating sums of squares, degrees of freedom, and expected mean squares are presented. It is demonstrated that all statistics needed for inference on linear combinations of parameters of a linear model may be calculated from the triangular reduction of the data matrix.},
 author = {Del T. Scott and G. Rex Bryce and David M. Allen},
 journal = {The American Statistician},
 number = {2},
 pages = {128--135},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Orthogonalization-Triangularization Methods in Statistical Computations},
 volume = {39},
 year = {1985}
}

@article {tibshirani.2011,
    AUTHOR = {Tibshirani, Robert},
     TITLE = {Regression shrinkage and selection via the lasso: a
              retrospective},
   JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
    VOLUME = {73},
      YEAR = {2011},
    NUMBER = {3},
     PAGES = {273--282},
      ISSN = {1369-7412},
   MRCLASS = {62J07 (62J02)},
  MRNUMBER = {2815776},
MRREVIEWER = {B. M. Golam Kibria},
       DOI = {10.1111/j.1467-9868.2011.00771.x},
       URL = {https://doi.org/10.1111/j.1467-9868.2011.00771.x},
}
		

@article {tibshirani.1996,
    AUTHOR = {Tibshirani, Robert},
     TITLE = {Regression shrinkage and selection via the lasso},
   JOURNAL = {J. Roy. Statist. Soc. Ser. B},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Methodological},
    VOLUME = {58},
      YEAR = {1996},
    NUMBER = {1},
     PAGES = {267--288},
      ISSN = {0035-9246},
   MRCLASS = {62J05 (62J07)},
  MRNUMBER = {1379242},
       URL =
              {http://links.jstor.org/sici?sici=0035-9246(1996)58:1<267:RSASVT>2.0.CO;2-G&origin=MSN},
}
		
@article {zou_hastie.2005,
author = {Zou, Hui and Hastie, Trevor},
title = {Regularization and variable selection via the elastic net},
JOURNAL = {J. Roy. Statist. Soc. Ser. B},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Methodological},
volume = {67},
number = {2},
publisher = {Blackwell Publishing Ltd},
issn = {1467-9868},
url = {http://dx.doi.org/10.1111/j.1467-9868.2005.00503.x},
doi = {10.1111/j.1467-9868.2005.00503.x},
pages = {301--320},
keywords = {Grouping effect, LARS algorithm, Lasso, Penalization, pâ«n problem, Variable selection},
year = {2005},
abstract = {Summary.â We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the pâ«n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
}

@article{zho.2006,
author = {Hui Zou},
title = {The Adaptive Lasso and Its Oracle Properties},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
volume = {101},
number = {476},
pages = {1418-1429},
year = {2006},
doi = {10.1198/016214506000000735},
URL = {http://dx.doi.org/10.1198/016214506000000735},
eprint = {http://dx.doi.org/10.1198/016214506000000735}
}

@article {yuan_lin.2006,
    AUTHOR = {Yuan, Ming and Lin, Yi},
     TITLE = {Model selection and estimation in regression with grouped
              variables},
   JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
    VOLUME = {68},
      YEAR = {2006},
    NUMBER = {1},
     PAGES = {49--67},
      ISSN = {1369-7412},
   MRCLASS = {62H25 (62M20)},
  MRNUMBER = {2212574},
       DOI = {10.1111/j.1467-9868.2005.00532.x},
       URL = {https://doi.org/10.1111/j.1467-9868.2005.00532.x},
}

@article {park_casella.2008,
    AUTHOR = {Park, Trevor and Casella, George},
     TITLE = {The {B}ayesian lasso},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {103},
      YEAR = {2008},
    NUMBER = {482},
     PAGES = {681--686},
      ISSN = {0162-1459},
   MRCLASS = {Expansion},
  MRNUMBER = {2524001},
       DOI = {10.1198/016214508000000337},
       URL = {https://doi.org/10.1198/016214508000000337},
}
  
@Article{hans.2010,
author="Hans, Chris",
title="Model uncertainty and variable selection in Bayesian lasso regression",
journal="Statistics and Computing",
year="2009",
volume="20",
number="2",
pages="221--229",
abstract="While Bayesian analogues of lasso regression have become popular, comparatively little has been said about formal treatments of model uncertainty in such settings. This paper describes methods that can be used to evaluate the posterior distribution over the space of all possible regression models for Bayesian lasso regression. Access to the model space posterior distribution is necessary if model-averaged inference---e.g., model-averaged prediction and calculation of posterior variable inclusion probabilities---is desired. The key element of all such inference is the ability to evaluate the marginal likelihood of the data under a given regression model, which has so far proved difficult for the Bayesian lasso. This paper describes how the marginal likelihood can be accurately computed when the number of predictors in the model is not too large, allowing for model space enumeration when the total number of possible predictors is modest. In cases where the total number of possible predictors is large, a simple Markov chain Monte Carlo approach for sampling the model space posterior is provided. This Gibbs sampling approach is similar in spirit to the stochastic search variable selection methods that have become one of the main tools for addressing Bayesian regression model uncertainty, and the adaption of these methods to the Bayesian lasso is shown to be straightforward.",
issn="1573-1375",
doi="10.1007/s11222-009-9160-9",
url="http://dx.doi.org/10.1007/s11222-009-9160-9"
}

@article {hans.2009,
    AUTHOR = {Hans, Chris},
     TITLE = {Bayesian lasso regression},
   JOURNAL = {Biometrika},
  FJOURNAL = {Biometrika},
    VOLUME = {96},
      YEAR = {2009},
    NUMBER = {4},
     PAGES = {835--845},
      ISSN = {0006-3444},
   MRCLASS = {62J07 (62F15)},
  MRNUMBER = {2564494},
       DOI = {10.1093/biomet/asp047},
       URL = {https://doi.org/10.1093/biomet/asp047},
}  
  
@article{pericchi_smith.1992,
     jstor_articletype = {research-article},
     title = {Exact and Approximate Posterior Moments for a Normal Location Parameter},
     author = {Pericchi, L. R. and Smith, A. F. M.},
     journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
     jstor_issuetitle = {},
     volume = {54},
     number = {3},
     jstor_formatteddate = {1992},
     pages = {pp. 793-804},
     url = {http://www.jstor.org/stable/2345859},
     ISSN = {00359246},
     language = {English},
     year = {1992},
     publisher = {Wiley for the Royal Statistical Society},
     copyright = {Copyright Â© 1992 Royal Statistical Society},
    }
  
  @Article{leng_etal.2013,
author="Leng, Chenlei
and Tran, Minh-Ngoc
and Nott, David",
title="Bayesian adaptive Lasso",
journal="Annals of the Institute of Statistical Mathematics",
year="2013",
volume="66",
number="2",
pages="221--244",
abstract="We propose the Bayesian adaptive Lasso (BaLasso) for variable selection and coefficient estimation in linear regression. The BaLasso is adaptive to the signal level by adopting different shrinkage for different coefficients. Furthermore, we provide a model selection machinery for the BaLasso by assessing the posterior conditional mode estimates, motivated by the hierarchical Bayesian interpretation of the Lasso. Our formulation also permits prediction using a model averaging strategy. We discuss other variants of this new approach and provide a unified framework for variable selection using flexible penalties. Empirical evidence of the attractiveness of the method is demonstrated via extensive simulation studies and data analysis.",
issn="1572-9052",
doi="10.1007/s10463-013-0429-6",
url="http://dx.doi.org/10.1007/s10463-013-0429-6"
}
  
@article{hahn_carvalho.2015,
author = {P. Richard Hahn and Carlos M. Carvalho},
title = {Decoupling Shrinkage and Selection in Bayesian Linear Models: A Posterior Summary Perspective},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
volume = {110},
number = {509},
pages = {435-448},
year = {2015},
doi = {10.1080/01621459.2014.993077},
URL = {http://dx.doi.org/10.1080/01621459.2014.993077},
eprint = {http://dx.doi.org/10.1080/01621459.2014.993077}
}
  
@article{lockhart_etal.2014,
author = "Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert",
doi = "10.1214/13-AOS1175",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "04",
number = "2",
pages = "413--468",
publisher = "The Institute of Mathematical Statistics",
title = "A significance test for the lasso",
url = "http://dx.doi.org/10.1214/13-AOS1175",
volume = "42",
year = "2014"
}  
  
@Article{lykou_ntzoufras.2013,
author="Lykou, Anastasia
and Ntzoufras, Ioannis",
title="On Bayesian lasso variable selection and the specification of the shrinkage parameter",
journal="Statistics and Computing",
year="2012",
volume="23",
number="3",
pages="361--390",
abstract="We propose a Bayesian implementation of the lasso regression that accomplishes both shrinkage and variable selection. We focus on the appropriate specification for the shrinkage parameter $\lambda$ through Bayes factors that evaluate the inclusion of each covariate in the model formulation. We associate this parameter with the values of Pearson and partial correlation at the limits between significance and insignificance as defined by Bayes factors. In this way, a meaningful interpretation of $\lambda$ is achieved that leads to a simple specification of this parameter. Moreover, we use these values to specify the parameters of a gamma hyperprior for $\lambda$. The parameters of the hyperprior are elicited such that appropriate levels of practical significance of the Pearson correlation are achieved and, at the same time, the prior support of $\lambda$ values that activate the Lindley-Bartlett paradox or lead to over-shrinkage of model coefficients is avoided. The proposed method is illustrated using two simulation studies and a real dataset. For the first simulation study, results for different prior values of $\lambda$ are presented as well as a detailed robustness analysis concerning the parameters of the hyperprior of $\lambda$. In all examples, detailed comparisons with a variety of ordinary and Bayesian lasso methods are presented.",
issn="1573-1375",
doi="10.1007/s11222-012-9316-x",
url="http://dx.doi.org/10.1007/s11222-012-9316-x"
}  
    
@article {gu_yamashita.2021,
    AUTHOR = {Gu, Yan and Yamashita, Nobuo},
     TITLE = {A proximal {ADMM} with the {B}royden family for convex
              optimization problems},
   JOURNAL = {J. Ind. Manag. Optim.},
  FJOURNAL = {Journal of Industrial and Management Optimization},
    VOLUME = {17},
      YEAR = {2021},
    NUMBER = {5},
     PAGES = {2715--2732},
      ISSN = {1547-5816},
   MRCLASS = {90C25 (65K05 90C53)},
  MRNUMBER = {4274440},
       DOI = {10.3934/jimo.2020091},
       URL = {https://doi.org/10.3934/jimo.2020091},
} 
  
@article{glowinski_marroco.1975,
	author = {Glowinski, R. and Marroco, A.},
	title = {Sur l'approximation, par \'el\'ements finis d'ordre un, et la r\'esolution, par p\'enalisation-dualit\'e d'une classe de probl\`emes de Dirichlet non lin\'eaires},
	DOI= "10.1051/m2an/197509R200411",
	url= "https://doi.org/10.1051/m2an/197509R200411",
	journal = {R.A.I.R.O. Analyse Num\'erique},
	year = 1975,
	volume = 9,
	number = R2,
	pages = "41-76",
}  
  
@article{gabay_mercier.1976,
title = {A dual algorithm for the solution of nonlinear variational problems via finite element approximation},
journal = {Computers \& Mathematics with Applications},
volume = {2},
number = {1},
pages = {17-40},
year = {1976},
issn = {0898-1221},
doi = {https://doi.org/10.1016/0898-1221(76)90003-1},
url = {https://www.sciencedirect.com/science/article/pii/0898122176900031},
author = {Daniel Gabay and Bertrand Mercier},
abstract = {For variational problems of the form InfvâV{f(Av)+g(v)}, we propose a dual method which decouples the difficulties relative to the functionals f and g from the possible ill-conditioning effects of the linear operator A. The approach is based on the use of an Augmented Lagrangian functional and leads to an efficient and simply implementable algorithm. We study also the finite element approximation of such problems, compatible with the use of our algorithm. The method is finally applied to solve several problems of continuum mechanics.}
}  
  

  
@article {egidi_maponi.2006,
    AUTHOR = {Egidi, N. and Maponi, P.},
     TITLE = {A {S}herman-{M}orrison approach to the solution of linear
              systems},
   JOURNAL = {J. Comput. Appl. Math.},
  FJOURNAL = {Journal of Computational and Applied Mathematics},
    VOLUME = {189},
      YEAR = {2006},
    NUMBER = {1-2},
     PAGES = {703--718},
      ISSN = {0377-0427},
   MRCLASS = {65F05},
  MRNUMBER = {2203008},
       DOI = {10.1016/j.cam.2005.02.013},
       URL = {https://doi.org/10.1016/j.cam.2005.02.013},
}  
  
 @book {lange.2010,
    AUTHOR = {Lange, Kenneth},
     TITLE = {Numerical analysis for statisticians},
    SERIES = {Statistics and Computing},
   EDITION = {Second},
 PUBLISHER = {Springer, New York},
      YEAR = {2010},
     PAGES = {xx+600},
      ISBN = {978-1-4419-5944-7},
   MRCLASS = {62-01 (65-01)},
  MRNUMBER = {2655999},
MRREVIEWER = {Dragos Calitoiu},
       DOI = {10.1007/978-1-4419-5945-4},
       URL = {https://doi.org/10.1007/978-1-4419-5945-4},
}



@book {lange.2016,
    AUTHOR = {Lange, Kenneth},
     TITLE = {M{M} optimization algorithms},
 PUBLISHER = {Society for Industrial and Applied Mathematics, Philadelphia,
              PA},
      YEAR = {2016},
     PAGES = {ix+223},
      ISBN = {978-1-611974-39-3},
   MRCLASS = {62-01 (26D15 49-02 62F10 62Hxx 62J05 90-02)},
  MRNUMBER = {3522165},
MRREVIEWER = {Anthony Almudevar},
       DOI = {10.1137/1.9781611974409.ch1},
       URL = {https://doi.org/10.1137/1.9781611974409.ch1},
}

@article {lange_etal.2014,
    AUTHOR = {Lange, Kenneth and Chi, Eric C. and Zhou, Hua},
     TITLE = {A brief survey of modern optimization for statisticians},
   JOURNAL = {Int. Stat. Rev.},
  FJOURNAL = {International Statistical Review. Revue Internationale de
              Statistique},
    VOLUME = {82},
      YEAR = {2014},
    NUMBER = {1},
     PAGES = {46--70},
      ISSN = {0306-7734},
   MRCLASS = {90-02 (62-02 62F10 65H10 90C06)},
  MRNUMBER = {3200532},
       DOI = {10.1111/insr.12022},
       URL = {https://doi.org/10.1111/insr.12022},
}

@article {hunter_lange.2004,
    AUTHOR = {Hunter, David R. and Lange, Kenneth},
     TITLE = {A tutorial on {MM} algorithms},
   JOURNAL = {Amer. Statist.},
  FJOURNAL = {The American Statistician},
    VOLUME = {58},
      YEAR = {2004},
    NUMBER = {1},
     PAGES = {30--37},
      ISSN = {0003-1305},
   MRCLASS = {Expansion},
  MRNUMBER = {2055509},
       DOI = {10.1198/0003130042836},
       URL = {https://doi.org/10.1198/0003130042836},
}

@book {lange.2013,
    AUTHOR = {Lange, Kenneth},
     TITLE = {Optimization},
    SERIES = {Springer Texts in Statistics},
    VOLUME = {95},
   EDITION = {Second},
 PUBLISHER = {Springer, New York},
      YEAR = {2013},
     PAGES = {xviii+529},
      ISBN = {978-1-4614-5837-1; 978-1-4614-5838-8},
   MRCLASS = {90-01 (62H25 62J05 90Cxx)},
  MRNUMBER = {3052733},
       DOI = {10.1007/978-1-4614-5838-8},
       URL = {https://doi.org/10.1007/978-1-4614-5838-8},
} 
  
@article {luk_qiao.1989,
    AUTHOR = {Luk, Franklin T. and Qiao, San Zheng},
     TITLE = {Analysis of a recursive least-squares signal-processing
              algorithm},
   JOURNAL = {SIAM J. Sci. Statist. Comput.},
  FJOURNAL = {Society for Industrial and Applied Mathematics. Journal on
              Scientific and Statistical Computing},
    VOLUME = {10},
      YEAR = {1989},
    NUMBER = {3},
     PAGES = {407--418},
      ISSN = {0196-5204},
   MRCLASS = {65F99},
  MRNUMBER = {991733},
       DOI = {10.1137/0910027},
       URL = {https://doi.org/10.1137/0910027},
}
		

@article {alexander_etal.1988,
    AUTHOR = {Alexander, S. T. and Pan, C.-T. and Plemmons, R. J.},
     TITLE = {Analysis of a recursive least squares hyperbolic rotation
              algorithm for signal processing},
   JOURNAL = {Linear Algebra Appl.},
  FJOURNAL = {Linear Algebra and its Applications},
    VOLUME = {98},
      YEAR = {1988},
     PAGES = {3--40},
      ISSN = {0024-3795},
   MRCLASS = {94A05 (94A11)},
  MRNUMBER = {919109},
MRREVIEWER = {D. Stanomir},
       DOI = {10.1016/0024-3795(88)90158-9},
       URL = {https://doi.org/10.1016/0024-3795(88)90158-9},
}
		
@article {ellingsen_leathrum.1975,
    AUTHOR = {Ellingsen, Walter R. and Leathrum, James F.},
     TITLE = {On-line ridge regression; sequential biased estimation for
              nonorthogonal problems},
   JOURNAL = {J. Statist. Comput. Simulation},
  FJOURNAL = {Journal of Statistical Computation and Simulation},
    VOLUME = {3},
      YEAR = {1975},
     PAGES = {249--264},
      ISSN = {0094-9655},
   MRCLASS = {62L12 (62J05)},
  MRNUMBER = {391439},
MRREVIEWER = {Colin L. Mallows},
       DOI = {10.1080/00949657508810090},
       URL = {https://doi.org/10.1080/00949657508810090},
}

@article{hoerl_kennard.2000,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1271436},
 abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X?X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X?X to obtain biased estimates with smaller mean square error.},
 author = {Arthur E. Hoerl and Robert W. Kennard},
 journal = {Technometrics},
 number = {1},
 pages = {80--86},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Ridge regression: biased estimation for nonorthogonal problems},
 urldate = {2022-04-11},
 volume = {42},
 year = {2000}
}

@article{ni_etal.2012,
author = {Ni, Jian and Neslin, Scott A. and Sun, Baohong},
title = {Database Submission—The ISMS Durable Goods Data Sets},
journal = {Marketing Science},
volume = {31},
number = {6},
pages = {1008-1013},
year = {2012},
doi = {10.1287/mksc.1120.0726},

URL = { 
        https://doi.org/10.1287/mksc.1120.0726
    
},
eprint = { 
        https://doi.org/10.1287/mksc.1120.0726}
}

@article{liu_rockova.2021,
author = {Yi Liu and Veronika Ro\v{c}kov\'{a}},
title = {Variable Selection Via Thompson Sampling},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
volume = {0},
number = {0},
pages = {1-18},
year  = {2021},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2021.1928514},
URL = {https://doi.org/10.1080/01621459.2021.192851},
eprint = {https://doi.org/10.1080/01621459.2021.1928514}
}

@article{bay_etal.2022,
author = {Ray Bai and Gemma E. Moran and Joseph L. Antonelli and Yong Chen and Mary R. Boland},
title = {Spike-and-Slab group lassos for grouped regression and sparse generalized additive models},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
volume = {117},
number = {537},
pages = {184-197},
year  = {2022},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2020.1765784},

URL = { 
        https://doi.org/10.1080/01621459.2020.1765784
    
},
eprint = { 
        https://doi.org/10.1080/01621459.2020.1765784
    
}

}

@misc{koop_korobilis.2018,
  doi = {10.48550/ARXIV.1809.03031},
  url = {https://arxiv.org/abs/1809.03031},
  author = {Koop, Gary and Korobilis, Dimitris},
  keywords = {Computation (stat.CO), Econometrics (econ.EM), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Economics and business, FOS: Economics and business},
  title = {Bayesian dynamic variable selection in high dimensions},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@Article{bernardi_etal.2016,
author={Bernardi, Mauro
and Casarin, Roberto
and Maillet, Bertrand B.
and Petrella, Lea},
title={Bayesian dynamic quantile model averaging},
journal={Annals of Operations Research},
year={2024},
month={Nov},
day={21},
abstract={This article introduces a novel dynamic framework to Bayesian model averaging for time-varying parameter quantile regressions. By employing sequential Markov chain Monte Carlo, we combine empirical estimates derived from dynamically chosen quantile regressions, thereby facilitating a comprehensive understanding of the quantile model instabilities. The effectiveness of our methodology is initially validated through the examination of simulated datasets and, subsequently, by two applications to the US inflation rates and to the US real estate market. Our empirical findings suggest that a more intricate and nuanced analysis is needed when examining different sub-period regimes, since the determinants of inflation and real estate prices are clearly shown to be time-varying. In conclusion, we suggest that our proposed approach could offer valuable insights to aid decision making in a rapidly changing environment.},
issn={1572-9338},
doi={10.1007/s10479-024-06378-7},
url={https://doi.org/10.1007/s10479-024-06378-7}
}

@article{mccracken_ng.2016,
 ISSN = {07350015},
 URL = {http://www.jstor.org/stable/44166591},
 abstract = {This article describes a large, monthly frequency, macroeconomic database with the goal of establishing a convenient starting point for empirical analysis that requires "big data." The dataset mimics the coverage of those already used in the literature but has three appealing features. First, it is designed to be updated monthly using the Federal Reserve Economic Data (FRED) database. Second, it will be publicly accessible, facilitating comparison of related research and replication of empirical work. Third, it will relieve researchers from having to manage data changes and revisions. We show that factors extracted from our dataset share the same predictive content as those based on various vintages of the so-called Stock-Watson dataset. In addition, we suggest that diffusion indexes constructed as the partial sum of the factor estimates can potentially be useful for the study of business cycle chronology. Supplementary materials for this article are available online.},
 author = {Michael W. McCracken and Serena Ng},
 journal = {Journal of Business \& Economic Statistics},
 number = {4},
 pages = {574--589},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {FRED-MD: A Monthly Database for Macroeconomic Research},
 urldate = {2022-10-28},
 volume = {34},
 year = {2016}
}



@book {deboor.2001,
    AUTHOR = {de Boor, Carl},
     TITLE = {A practical guide to splines},
    SERIES = {Applied Mathematical Sciences},
    VOLUME = {27},
   EDITION = {Revised},
 PUBLISHER = {Springer-Verlag, New York},
      YEAR = {2001},
     PAGES = {xviii+346},
      ISBN = {0-387-95366-3},
   MRCLASS = {41-01 (41A15 65D05 65D07 65D10)},
  MRNUMBER = {1900298},
MRREVIEWER = {Gerlind Plonka},
}

@book {wood.2017,
    AUTHOR = {Wood, Simon N.},
     TITLE = {Generalized Additive Models: An introduction with R},
 PUBLISHER = {CRC Press, Boca Raton, FL},
      YEAR = {2017},
     PAGES = {xx+476},
      ISBN = {978-1-4987-2833-1},
   MRCLASS = {62-01 (62-04 62G05 62J02 62J05 62J12)},
  MRNUMBER = {3726911},
MRREVIEWER = {Wolfgang Wefelmeyer},
}

@article{gatu_etal.2003,
    AUTHOR = {Gatu, Cristian and Kontoghiorghes, Erricos J.},
     TITLE = {Parallel algorithms for computing all possible subset
              regression models using the {QR} decomposition},
      NOTE = {Parallel computing in numerical optimization (Naples, 2001)},
   JOURNAL = {Parallel Comput.},
  FJOURNAL = {Parallel Computing. Systems \& Applications},
    VOLUME = {29},
      YEAR = {2003},
    NUMBER = {4},
     PAGES = {505--521},
      ISSN = {0167-8191},
   MRCLASS = {90C20 (62G08 65C60 65Y05)},
  MRNUMBER = {1987227},
       DOI = {10.1016/S0167-8191(03)00019-X},
       URL = {https://doi.org/10.1016/S0167-8191(03)00019-X},
}

@article {kontoghiorghes.1999,
    AUTHOR = {Kontoghiorghes, E. J.},
     TITLE = {Parallel strategies for computing the orthogonal
              factorizations used in the estimation of econometric models},
   JOURNAL = {Algorithmica},
  FJOURNAL = {Algorithmica. An International Journal in Computer Science},
    VOLUME = {25},
      YEAR = {1999},
    NUMBER = {1},
     PAGES = {58--74},
      ISSN = {0178-4617},
   MRCLASS = {62P20},
  MRNUMBER = {1700463},
       DOI = {10.1007/PL00009283},
       URL = {https://doi.org/10.1007/PL00009283},
}

@article{hofmann_etal.2007,
title = {Efficient algorithms for computing the best subset regression models for large-scale problems},
journal = {Computational Statistics \& Data Analysis},
volume = {52},
number = {1},
pages = {16-29},
year = {2007},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2007.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167947307001168},
author = {Marc Hofmann and Cristian Gatu and Erricos John Kontoghiorghes},
keywords = {Best-subset regression, Regression tree, Branch-and-bound algorithm},
abstract = {Several strategies for computing the best subset regression models are proposed. Some of the algorithms are modified versions of existing regression-tree methods, while others are new. The first algorithm selects the best subset models within a given size range. It uses a reduced search space and is found to outperform computationally the existing branch-and-bound algorithm. The properties and computational aspects of the proposed algorithm are discussed in detail. The second new algorithm preorders the variables inside the regression tree. A radius is defined in order to measure the distance of a node from the root of the tree. The algorithm applies the preordering to all nodes which have a smaller distance than a certain radius that is given a priori. An efficient method of preordering the variables is employed. The experimental results indicate that the algorithm performs best when preordering is employed on a radius of between one quarter and one third of the number of variables. The algorithm has been applied with such a radius to tackle large-scale subset-selection problems that are considered to be computationally infeasible by conventional exhaustive-selection methods. A class of new heuristic strategies is also proposed. The most important of these is one that assigns a different tolerance value to each subset model size. This strategy with different kind of tolerances is equivalent to all exhaustive and heuristic subset-selection strategies. In addition the strategy can be used to investigate submodels having noncontiguous size ranges. Its implementation provides a flexible tool for tackling large scale models.}
}

@article {ghosh_clyde.2011,
    AUTHOR = {Ghosh, Joyee and Clyde, Merlise A.},
     TITLE = {Rao-{B}lackwellization for {B}ayesian variable selection and
              model averaging in linear and binary regression: a novel data
              augmentation approach},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {106},
      YEAR = {2011},
    NUMBER = {495},
     PAGES = {1041--1052},
      ISSN = {0162-1459},
   MRCLASS = {62F15 (62F07)},
  MRNUMBER = {2894762},
MRREVIEWER = {Radu V. Craiu},
       DOI = {10.1198/jasa.2011.tm10518},
       URL = {https://doi.org/10.1198/jasa.2011.tm10518},
}

@article {johnson_rossell.2012,
    AUTHOR = {Johnson, Valen E. and Rossell, David},
     TITLE = {Bayesian model selection in high-dimensional settings},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {107},
      YEAR = {2012},
    NUMBER = {498},
     PAGES = {649--660},
      ISSN = {0162-1459,1537-274X},
   MRCLASS = {62F15 (62J07)},
  MRNUMBER = {2980074},
MRREVIEWER = {Yuichi\ Hirose},
       DOI = {10.1080/01621459.2012.682536},
       URL = {https://doi.org/10.1080/01621459.2012.682536},
}

@article {narisetty_etal.2014,
    AUTHOR = {Narisetty, Naveen Naidu and He, Xuming},
     TITLE = {Bayesian variable selection with shrinking and diffusing
              priors},
   JOURNAL = {Ann. Statist.},
  FJOURNAL = {The Annals of Statistics},
    VOLUME = {42},
      YEAR = {2014},
    NUMBER = {2},
     PAGES = {789--817},
      ISSN = {0090-5364},
   MRCLASS = {62J05 (62F12 62F15)},
  MRNUMBER = {3210987},
MRREVIEWER = {Jialiang Li},
       DOI = {10.1214/14-AOS1207},
       URL = {https://doi.org/10.1214/14-AOS1207},
}

@article {narisetty_etal.2019,
    AUTHOR = {Narisetty, Naveen N. and Shen, Juan and He, Xuming},
     TITLE = {Skinny {G}ibbs: a consistent and scalable {G}ibbs sampler for
              model selection},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {114},
      YEAR = {2019},
    NUMBER = {527},
     PAGES = {1205--1217},
      ISSN = {0162-1459},
   MRCLASS = {62F15 (62J12)},
  MRNUMBER = {4011773},
MRREVIEWER = {The Tien Mai},
       DOI = {10.1080/01621459.2018.1482754},
       URL = {https://doi.org/10.1080/01621459.2018.1482754},
}

@incollection {narisetty.2020,
    AUTHOR = {Narisetty, Naveen Naidu},
     TITLE = {Bayesian model selection for high-dimensional data},
 BOOKTITLE = {Principles and methods for data science},
    SERIES = {Handbook of Statist.},
    VOLUME = {43},
     PAGES = {207--248},
 PUBLISHER = {Elsevier/North-Holland, Amsterdam},
      YEAR = {[2020] \copyright 2020},
   MRCLASS = {62H30 (62F15)},
  MRNUMBER = {4254233},
       DOI = {10.1016/bs.host.2019.08.001},
       URL = {https://doi.org/10.1016/bs.host.2019.08.001},
}

@inproceedings{biswas_etal.2022,
  title={Scalable spike-and-slab},
  author={Biswas, Niloy and Mackey, Lester and Meng, Xiao-Li},
  booktitle={International Conference on Machine Learning},
  pages={2021--2040},
  year={2022},
  organization={PMLR}
}

@article {ray_etal.2022,
    AUTHOR = {Ray, Kolyan and Szab\'{o}, Botond},
     TITLE = {Variational {B}ayes for {H}igh-{D}imensional {L}inear
              {R}egression {W}ith {S}parse {P}riors},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {117},
      YEAR = {2022},
    NUMBER = {539},
     PAGES = {1270--1281},
      ISSN = {0162-1459},
   MRCLASS = {Prelim},
  MRNUMBER = {4480711},
       DOI = {10.1080/01621459.2020.1847121},
       URL = {https://doi.org/10.1080/01621459.2020.1847121},
}

@article{hesterberg_etal.2018,
author = {Tim Hesterberg and Nam Hee Choi and Lukas Meier and Chris Fraley},
title = {Least angle and $\ell_1$ penalized regression: A reivew},
volume = {2},
journal = {Statistics Surveys},
number = {none},
publisher = {Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
pages = {61 -- 93},
keywords = {ℓ_1 penalty, Lasso, regression, regularization, Variable selection},
year = {2008},
doi = {10.1214/08-SS035},
URL = {https://doi.org/10.1214/08-SS035}
}

@article {efron_etal.2004,
    AUTHOR = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and
              Tibshirani, Robert},
     TITLE = {Least angle regression},
   JOURNAL = {Ann. Statist.},
  FJOURNAL = {The Annals of Statistics},
    VOLUME = {32},
      YEAR = {2004},
    NUMBER = {2},
     PAGES = {407--499},
      ISSN = {0090-5364},
   MRCLASS = {62J07},
  MRNUMBER = {2060166},
MRREVIEWER = {Holger Dette},
       DOI = {10.1214/009053604000000067},
       URL = {https://doi.org/10.1214/009053604000000067},
}

@article {zou_hastie.2005a,
    AUTHOR = {Zou, Hui and Hastie, Trevor},
     TITLE = {Addendum: ``{R}egularization and variable selection via the
              elastic net'' [{J}. {R}. {S}tat. {S}oc. {S}er. {B} {S}tat.
              {M}ethodol. {\bf 67} (2005), no. 2, 301--320; MR2137327]},
   JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
    VOLUME = {67},
      YEAR = {2005},
    NUMBER = {5},
     PAGES = {768},
      ISSN = {1369-7412},
   MRCLASS = {62M20},
  MRNUMBER = {2210692},
       DOI = {10.1111/j.1467-9868.2005.00527.x},
       URL = {https://doi.org/10.1111/j.1467-9868.2005.00527.x},
}
		

@article {zou_hastie.2005b,
    AUTHOR = {Zou, Hui and Hastie, Trevor},
     TITLE = {Regularization and variable selection via the elastic net},
   JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
    VOLUME = {67},
      YEAR = {2005},
    NUMBER = {2},
     PAGES = {301--320},
      ISSN = {1369-7412},
   MRCLASS = {62M20},
  MRNUMBER = {2137327},
       DOI = {10.1111/j.1467-9868.2005.00503.x},
       URL = {https://doi.org/10.1111/j.1467-9868.2005.00503.x},
}
		
@article{friedman_etal.2010,
 title={Regularization Paths for Generalized Linear Models via Coordinate Descent},
 volume={33},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v033i01},
 doi={10.18637/jss.v033.i01},
 abstract={We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multi- nomial regression problems while the penalties include ℓ&amp;lt;sub&amp;gt;1&amp;lt;/sub&amp;gt; (the lasso), ℓ&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt; (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
 number={1},
 journal={Journal of Statistical Software},
 author={Friedman, Jerome H. and Hastie, Trevor and Tibshirani, Rob},
 year={2010},
 pages={1–22}
}

@article{scheetz_etal.2006,
author = {Todd E. Scheetz  and Kwang-Youn A. Kim  and Ruth E. Swiderski  and Alisdair R. Philp  and Terry A. Braun  and Kevin L. Knudtson  and Anne M. Dorrance  and Gerald F. DiBona  and Jian Huang  and Thomas L. Casavant  and Val C. Sheffield  and Edwin M. Stone },
title = {Regulation of gene expression in the mammalian eye and its relevance to eye disease},
fjournal = {Proceedings of the National Academy of Sciences},
journal={Proc. Natl. Acad. Sci. USA.},
volume = {103},
number = {39},
pages = {14429-14434},
year = {2006},
doi = {10.1073/pnas.0602562103},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0602562103},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0602562103},
abstract = {We used expression quantitative trait locus mapping in the laboratory rat (Rattus norvegicus) to gain a broad perspective of gene regulation in the mammalian eye and to identify genetic variation relevant to human eye disease. Of \&gt;31,000 gene probes represented on an Affymetrix expression microarray, 18,976 exhibited sufficient signal for reliable analysis and at least 2-fold variation in expression among 120 F2 rats generated from an SR/JrHsd × SHRSP intercross. Genome-wide linkage analysis with 399 genetic markers revealed significant linkage with at least one marker for 1,300 probes (α = 0.001; estimated empirical false discovery rate = 2\%). Both contiguous and noncontiguous loci were found to be important in regulating mammalian eye gene expression. We investigated one locus of each type in greater detail and identified putative transcription-altering variations in both cases. We found an inserted cREL binding sequence in the 5′ flanking sequence of the Abca4 gene associated with an increased expression level of that gene, and we found a mutation of the gene encoding thyroid hormone receptor β2 associated with a decreased expression level of the gene encoding short-wavelength sensitive opsin (Opn1sw). In addition to these positional studies, we performed a pairwise analysis of gene expression to identify genes that are regulated in a coordinated manner and used this approach to validate two previously undescribed genes involved in the human disease Bardet–Biedl syndrome. These data and analytical approaches can be used to facilitate the discovery of additional genes and regulatory elements involved in human eye disease.}}

@Article{breheny_huang.2015,
author={Breheny, Patrick
and Huang, Jian},
title={Group descent algorithms for nonconvex penalized linear and logistic regression models with grouped predictors},
journal={Statistics and Computing},
year={2015},
month={Mar},
day={01},
volume={25},
number={2},
pages={173-187},
abstract={Penalized regression is an attractive framework for variable selection problems. Often, variables possess a grouping structure, and the relevant selection problem is that of selecting groups, not individual variables. The group lasso has been proposed as a way of extending the ideas of the lasso to the problem of group selection. Nonconvex penalties such as SCAD and MCP have been proposed and shown to have several advantages over the lasso; these penalties may also be extended to the group selection problem, giving rise to group SCAD and group MCP methods. Here, we describe algorithms for fitting these models stably and efficiently. In addition, we present simulation results and real data examples comparing and contrasting the statistical properties of these methods.},
issn={1573-1375},
doi={10.1007/s11222-013-9424-2},
url={https://doi.org/10.1007/s11222-013-9424-2}
}

@article{chiang_etal.2006,
author = {Annie P. Chiang  and John S. Beck  and Hsan-Jan Yen  and Marwan K. Tayeh  and Todd E. Scheetz  and Ruth E. Swiderski  and Darryl Y. Nishimura  and Terry A. Braun  and Kwang-Youn A. Kim  and Jian Huang  and Khalil Elbedour  and Rivka Carmi  and Diane C. Slusarski  and Thomas L. Casavant  and Edwin M. Stone  and Val C. Sheffield },
title = {Homozygosity mapping with SNP arrays identifies {TRIM32}, an {E3} ubiquitin ligase, as a {B}ardet-{B}iedl syndrome gene ({BBS11})},
fjournal = {Proceedings of the National Academy of Sciences},
journal={Proc. Natl. Acad. Sci. USA.},
volume = {103},
number = {16},
pages = {6287-6292},
year = {2006},
doi = {10.1073/pnas.0600158103},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0600158103},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0600158103},
abstract = {The identification of mutations in genes that cause human diseases has largely been accomplished through the use of positional cloning, which relies on linkage mapping. In studies of rare diseases, the resolution of linkage mapping is limited by the number of available meioses and informative marker density. One recent advance is the development of high-density SNP microarrays for genotyping. The SNP arrays overcome low marker informativity by using a large number of markers to achieve greater coverage at finer resolution. We used SNP microarray genotyping for homozygosity mapping in a small consanguineous Israeli Bedouin family with autosomal recessive Bardet–Biedl syndrome (BBS; obesity, pigmentary retinopathy, polydactyly, hypogonadism, renal and cardiac abnormalities, and cognitive impairment) in which previous linkage studies using short tandem repeat polymorphisms failed to identify a disease locus. SNP genotyping revealed a homozygous candidate region. Mutation analysis in the region of homozygosity identified a conserved homozygous missense mutation in the TRIM32 gene, a gene coding for an E3 ubiquitin ligase. Functional analysis of this gene in zebrafish and expression correlation analyses among other BBS genes in an expression quantitative trait loci data set demonstrate that TRIM32 is a BBS gene. This study shows the value of high-density SNP genotyping for homozygosity mapping and the use of expression correlation data for evaluation of candidate genes and identifies the proteasome degradation pathway as a pathway involved in BBS.}}

@article{gagnon.2021,
author = {Philippe Gagnon},
title = {{Informed reversible jump algorithms}},
volume = {15},
journal = {Electronic Journal of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {3951 -- 3995},
keywords = {Bayesian statistics, large-sample asymptotics, Markov chain Monte Carlo methods, model averaging, Model selection, trans-dimensional samplers, Variable selection, weak convergence},
year = {2021},
doi = {10.1214/21-EJS1877},
URL = {https://doi.org/10.1214/21-EJS1877}
}

@article {narisetty_he.2014,
    AUTHOR = {Narisetty, Naveen Naidu and He, Xuming},
     TITLE = {Bayesian variable selection with shrinking and diffusing
              priors},
   JOURNAL = {Ann. Statist.},
  FJOURNAL = {The Annals of Statistics},
    VOLUME = {42},
      YEAR = {2014},
    NUMBER = {2},
     PAGES = {789--817},
      ISSN = {0090-5364},
   MRCLASS = {62J05 (62F12 62F15)},
  MRNUMBER = {3210987},
MRREVIEWER = {Jialiang Li},
       DOI = {10.1214/14-AOS1207},
       URL = {https://doi.org/10.1214/14-AOS1207},
}

@book {trefethen_bau.1997,
    AUTHOR = {Trefethen, Lloyd N. and Bau, III, David},
     TITLE = {Numerical linear algebra},
 PUBLISHER = {Society for Industrial and Applied Mathematics (SIAM),
              Philadelphia, PA},
      YEAR = {1997},
     PAGES = {xii+361},
      ISBN = {0-89871-361-7},
   MRCLASS = {65-01 (65-02 65Fxx)},
  MRNUMBER = {1444820},
MRREVIEWER = {Moody\ T.\ Chu},
       DOI = {10.1137/1.9780898719574},
       URL = {https://doi.org/10.1137/1.9780898719574},
}

@article{stoll.2020,
author = {Stoll, Martin},
title = {A literature survey of matrix methods for data science},
journal = {GAMM-Mitteilungen},
volume = {43},
number = {3},
pages = {e202000013},
keywords = {Data Science, Numerical linear algebra},
doi = {https://doi.org/10.1002/gamm.202000013},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/gamm.202000013},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/gamm.202000013},
abstract = {Abstract Efficient numerical linear algebra is a core ingredient in many applications across almost all scientific and industrial disciplines. With this survey we want to illustrate that numerical linear algebra has played and is playing a crucial role in enabling and improving data science computations with many new developments being fueled by the availability of data and computing resources. We highlight the role of various different factorizations and the power of changing the representation of the data as well as discussing topics such as randomized algorithms, functions of matrices, and high-dimensional problems. We briefly touch upon the role of techniques from numerical linear algebra used within deep learning.},
year = {2020}
}

@article{george_foster.2000,
    author = {George, EdwardI. and Foster, Dean P.},
    title = "{Calibration and empirical Bayes variable selection}",
    journal = {Biometrika},
    volume = {87},
    number = {4},
    pages = {731-747},
    year = {2000},
    month = {12},
    abstract = "{For the problem of variable selection for the normal linear model, selection criteria such as aic, Cp , bic and ric have fixed dimensionality penalties. Such criteria are shown to correspond to selection of maximum posterior models under implicit hyperparameter choices for a particular hierarchical Bayes formulation. Based on this calibration, we propose empirical Bayes selection criteria that use hyperparameter estimates instead of fixed choices. For obtaining these estimates, both marginal and conditional maximum likelihood methods are considered. As opposed to traditional fixed penalty criteria, these empirical Bayes criteria have dimensionality penalties that depend on the data. Their performance is seen to approximate adaptively the performance of the best fixed‐penalty criterion across a variety of orthogonal and nonorthogonal set‐ups, including wavelet regression. Empirical Bayes shrinkage estimators of the selected coefficients are also proposed.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/87.4.731},
    url = {https://doi.org/10.1093/biomet/87.4.731},
    eprint = {https://academic.oup.com/biomet/article-pdf/87/4/731/632465/870731.pdf},
}

@article{held_ott.2018,
   author = "Held, Leonhard and Ott, Manuela",
   title = "On p-values and Bayes factors", 
    fjournal= "Annual Review of Statistics and Its Application",
   journal="Annu. Rev. Stat. Its Appl.",
   year = "2018",
   volume = "5",
   number = "Volume 5, 2018",
   pages = "393-419",
   doi = "https://doi.org/10.1146/annurev-statistics-031017-100307",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031017-100307",
   publisher = "Annual Reviews",
   issn = "2326-831X",
   type = "Journal Article",
   keywords = "minimum Bayes factor",
   keywords = "objective Bayes",
   keywords = "evidence",
   keywords = "sample size",
   keywords = "Bayes factor",
   keywords = "p-value",
   abstract = "The p-value quantifies the discrepancy between the data and a null hypothesis of interest, usually the assumption of no difference or no effect. A Bayesian approach allows the calibration of p-values by transforming them to direct measures of the evidence against the null hypothesis, so-called Bayes factors. We review the available literature in this area and consider two-sided significance tests for a point null hypothesis in more detail. We distinguish simple from local alternative hypotheses and contrast traditional Bayes factors based on the data with Bayes factors based on p-values or test statistics. A well-known finding is that the minimum Bayes factor, the smallest possible Bayes factor within a certain class of alternative hypotheses, provides less evidence against the null hypothesis than the corresponding p-value might suggest. It is less known that the relationship between p-values and minimum Bayes factors also depends on the sample size and on the dimension of the parameter of interest. We illustrate the transformation of p-values to minimum Bayes factors with two examples from clinical research.",
  }

@article{fan_sisson.2011,
  title={Reversible jump MCMC},
  author={Fan, Yanan and Sisson, Scott A},
  journal={Handbook of Markov Chain Monte Carlo},
  pages={67--92},
  year={2011},
  publisher={Chapman and Hall/CRC}
}
  
@article{nguyen.2017,
author = {Nguyen, Hien D.},
title = {An introduction to Majorization-Minimization algorithms for machine learning and statistical estimation},
journal = {WIREs Data Mining and Knowledge Discovery},
volume = {7},
number = {2},
doi = {https://doi.org/10.1002/widm.1198},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1198},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1198},
abstract = {MM (majorization–minimization) algorithms are an increasingly popular tool for solving optimization problems in machine learning and statistical estimation. This article introduces the MM algorithm framework in general and via three commonly considered example applications: Gaussian mixture regressions, multinomial logistic regressions, and support vector machines. Specific algorithms for these three examples are derived and Mathematical Programming Series A numerical demonstrations are presented. Theoretical and practical aspects of MM algorithm design are discussed. WIREs Data Mining Knowl Discov 2017, 7:e1198. doi: 10.1002/widm.1198 This article is categorized under: Algorithmic Development > Statistics Technologies > Machine Learning},
year = {2017}
}  
  
@article{boyd_etal.2011,
author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
title = {Distributed {O}ptimization and {S}tatistical {L}earning via the {A}lternating {D}irection {M}ethod of {M}ultipliers},
year = {2011},
issue_date = {January 2011},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {3},
number = {1},
issn = {1935-8237},
journal = {Foundations and Trends® in Machine Learning},
month = {jan},
pages = {1–122},
numpages = {122}
}  
  
@article{chipman.2001,
  title={The practical implementation of Bayesian model selection},
  author={Chipman, Hugh and George, Edward I and McCulloch, Robert E and Clyde, Merlise and Foster, Dean P and Stine, Robert A},
  journal={Lecture Notes-Monograph Series},
  pages={65--134},
  year={2001},
  publisher={JSTOR}
}  
  
@article{jansen.2015,
title = {Generalized cross validation in variable selection with and without shrinkage},
JOURNAL = {J. Statist. Plann. Inference},
  FJOURNAL = {Journal of Statistical Planning and Inference},
volume = {159},
pages = {90-104},
year = {2015},
issn = {0378-3758},
doi = {https://doi.org/10.1016/j.jspi.2014.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0378375814001797},
author = {Maarten Jansen},
keywords = {Generalized Cross Validation, Variable selection, Threshold, Lasso, Mallows’s , Sparsity, High-dimensional data},
abstract = {This paper investigates two types of results that support the use of Generalized Cross Validation (GCV) for variable selection under the assumption of sparsity. The first type of result is based on the well established links between GCV on one hand and Mallows’s Cp and Stein Unbiased Risk Estimator (SURE) on the other hand. The result states that GCV performs as well as Cp or SURE in a regularized or penalized least squares problem as an estimator of the prediction error for the penalty in the neighborhood of its optimal value. This result can be seen as a refinement of an earlier result in GCV for soft thresholding of wavelet coefficients. The second novel result concentrates on the behavior of GCV for penalties near zero. Good behavior near zero is of crucial importance to ensure successful minimization of GCV as a function of the regularization parameter. Understanding the behavior near zero is important in the extension of GCV from ℓ1 towards ℓ0 regularized least squares, i.e., for variable selection without shrinkage, or hard thresholding. Several possible implementations of GCV are compared with each other and with SURE and Cp. These simulations illustrate the importance of the fact that GCV has an implicit and robust estimator of the observational variance.}
}

@article{jansen.2014,
    author = {Jansen, Maarten},
    title = "{Information criteria for variable selection under sparsity}",
    journal = {Biometrika},
    volume = {101},
    number = {1},
    pages = {37-55},
    year = {2014},
    month = {03},
    abstract = "{The optimization of an information criterion in a variable selection procedure leads to an additional bias, which can be substantial for sparse, high-dimensional data. One can compensate for the bias by applying shrinkage while estimating within the selected models. This paper presents modified information criteria for use in variable selection and estimation without shrinkage. The analysis motivating the modified criteria follows two routes. The first, which we explore for signal-plus-noise observations only, proceeds by comparing estimators with and without shrinkage. The second, discussed for general regression models, describes the optimization or selection bias as a double-sided effect, which we call a mirror effect: among the numerous insignificant variables, those with large, noisy values appear more valuable than an arbitrary variable, while in fact they carry more noise than an arbitrary variable. The mirror effect is investigated for Akaike’s information criterion and for Mallows’ Cp, with special attention paid to the latter criterion as a stopping rule in a least-angle regression routine. The result is a new stopping rule, which focuses not on the quality of a lasso shrinkage selection but on the least-squares estimator without shrinkage within the same selection.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/ast055},
    url = {https://doi.org/10.1093/biomet/ast055},
    eprint = {https://academic.oup.com/biomet/article-pdf/101/1/37/17461026/ast055.pdf},
}
  
@article{deshpande_etal.2019,
author = {Sameer K. Deshpande, Veronika Ročková and Edward I. George},
title = {Simultaneous variable and covariance selection vith the multivariate spike-and-slab LASSO},
JOURNAL = {J. Comput. Graph. Stat.},
  FJOURNAL = {Journal of Computational and Graphical Statistics},
volume = {28},
number = {4},
pages = {921--931},
year = {2019},
publisher = {ASA Website},
doi = {10.1080/10618600.2019.1593179},
URL = {https://doi.org/10.1080/10618600.2019.1593179},
eprint = {https://doi.org/10.1080/10618600.2019.1593179}
} 
  
@book {bauwens_etal.2000,
    AUTHOR = {Bauwens, Luc and Lubrano, Michel and Richard, Jean-Fran\c{c}ois},
     TITLE = {Bayesian inference in dynamic econometric models},
    SERIES = {Advanced Texts in Econometrics},
      NOTE = {With a foreword by Jacques H. Dr\`eze},
 PUBLISHER = {Oxford University Press, Oxford},
      YEAR = {1999},
     PAGES = {xvi+350},
      ISBN = {0-19-877313-7},
   MRCLASS = {62-01 (62F15 62P20 91B84)},
  MRNUMBER = {1858176},
MRREVIEWER = {Mark Steel},
}
  
@article {rothman_etal.2010a,
    AUTHOR = {Rothman, Adam J. and Levina, Elizaveta and Zhu, Ji},
     TITLE = {A new approach to {C}holesky-based covariance regularization
              in high dimensions},
   JOURNAL = {Biometrika},
  FJOURNAL = {Biometrika},
    VOLUME = {97},
      YEAR = {2010},
    NUMBER = {3},
     PAGES = {539--550},
      ISSN = {0006-3444},
   MRCLASS = {62H12 (15B48 62J07)},
  MRNUMBER = {2672482},
MRREVIEWER = {B. M. Golam Kibria},
}  
  
@article {rothman_etal.2010b,
    AUTHOR = {Rothman, Adam J. and Levina, Elizaveta and Zhu, Ji},
     TITLE = {Sparse multivariate regression with covariance estimation},
   JOURNAL = {J. Comput. Graph. Statist.},
  FJOURNAL = {Journal of Computational and Graphical Statistics},
    VOLUME = {19},
      YEAR = {2010},
    NUMBER = {4},
     PAGES = {947--962},
      ISSN = {1061-8600},
   MRCLASS = {62J05 (62J07)},
  MRNUMBER = {2791263},
}  

@article{xu_ghosh.2015,
author = {Xiaofan Xu and Malay Ghosh},
title = {{Bayesian Variable Selection and Estimation for Group Lasso}},
volume = {10},
journal = {Bayesian Analysis},
number = {4},
publisher = {International Society for Bayesian Analysis},
pages = {909 -- 936},
keywords = {Gibbs sampling, group variable selection, median thresholding, spike and slab prior},
year = {2015},
doi = {10.1214/14-BA929},
URL = {https://doi.org/10.1214/14-BA929}
}

@article{lai_chen.2021,
author = {Lai, Wei-Ting and Chen, Ray-Bing},
title = {A review of Bayesian group selection approaches for linear regression models},
journal = {WIREs Computational Statistics},
volume = {13},
number = {4},
pages = {e1513},
keywords = {Bayesian indicator selection, Bayesian LASSO, sparse group},
doi = {https://doi.org/10.1002/wics.1513},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1513},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1513},
abstract = {Abstract Grouping selection arises naturally in many statistical modeling problems. Several group selection methods have been proposed in the last two decades. In this paper, we review the Bayesian group selection approaches for linear regression models. We start from the Bayesian indicator approach and then move to the Bayesian group LASSO methods. In addition, we also consider the Bayesian methods for the sparse group selection that can be treated as an extension of the group selection. Finally, we mention some extensions of Bayesian group selection for the generalized linear models and the multiple response models. This article is categorized under: Statistical and Graphical Methods of Data Analysis > Dimension Reduction Statistical and Graphical Methods of Data Analysis > Bayesian Methods and Theory Statistical Models > Model Selection},
year = {2021}
}

@ARTICLE{eilers_marx.1996,
    author = {Paul H. C. Eilers and Dcmr Milieudienst Rijnmond and Brian D. Marx},
    title = {Flexible smoothing with B-splines and penalties},
    fjournal = {Statistical Science},
journal={Stat. Sci.},
    year = {1996},
    volume = {11},
    pages = {89--121}
}

@book{eilers_marx.2021,
  title={Practical Smoothing: The Joys of P-splines},
  author={Eilers, P.H.C. and Marx, B.D.},
  isbn={9781108482950},
  lccn={2020016638},
  url={https://books.google.it/books?id=ez0QEAAAQBAJ},
  year={2021},
  publisher={Cambridge University Press}
}

@article {bien_tibshirani.2011,
    AUTHOR = {Bien, Jacob and Tibshirani, Robert J.},
     TITLE = {Sparse estimation of a covariance matrix},
   JOURNAL = {Biometrika},
  FJOURNAL = {Biometrika},
    VOLUME = {98},
      YEAR = {2011},
    NUMBER = {4},
     PAGES = {807--820},
      ISSN = {0006-3444},
     CODEN = {BIOKAX},
   MRCLASS = {62J10 (62-09 62J07)},
  MRNUMBER = {2860325},
       DOI = {10.1093/biomet/asr054},
       URL = {http://dx.doi.org/10.1093/biomet/asr054}
}


@article {rothman.2012,
    AUTHOR = {Rothman, Adam J.},
     TITLE = {Positive definite estimators of large covariance matrices},
   JOURNAL = {Biometrika},
  FJOURNAL = {Biometrika},
    VOLUME = {99},
      YEAR = {2012},
    NUMBER = {3},
     PAGES = {733--740},
      ISSN = {0006-3444},
   MRCLASS = {62H12 (62H20 62H30 62J10)},
  MRNUMBER = {2966781},
MRREVIEWER = {Goetz Trenkler},
       DOI = {10.1093/biomet/ass025},
       URL = {https://doi.org/10.1093/biomet/ass025}
}

@Article{lamnisos_etal.2012,
author={Lamnisos, D.
and Griffin, J. E.
and Steel, M. F. J.},
title={Cross-validation prior choice in Bayesian probit regression with many covariates},
journal={Statistics and Computing},
year={2012},
month={Mar},
day={01},
volume={22},
number={2},
pages={359-373},
abstract={This paper examines prior choice in probit regression through a predictive cross-validation criterion. In particular, we focus on situations where the number of potential covariates is far larger than the number of observations, such as in gene expression data. Cross-validation avoids the tendency of such models to fit perfectly. We choose the scale parameter c in the standard variable selection prior as the minimizer of the log predictive score. Naive evaluation of the log predictive score requires substantial computational effort, and we investigate computationally cheaper methods using importance sampling. We find that K-fold importance densities perform best, in combination with either mixing over different values of c or with integrating over c through an auxiliary distribution.},
issn={1573-1375},
doi={10.1007/s11222-011-9228-1},
url={https://doi.org/10.1007/s11222-011-9228-1}
}

@article{hojsgaard_lauritzen.2024,
    author = {Højsgaard, S and Lauritzen, S},
    title = "{On some algorithms for estimation in Gaussian graphical models}",
    journal = {Biometrika},
    year = {2024},
    month = {06},
    abstract = "{In Gaussian graphical models, the likelihood equations must typically be solved iteratively. This paper investigates two algorithms: a version of iterative proportional scaling, which avoids inversion of large matrices, and an algorithm based on convex duality and operating on the covariance matrix by neighbourhood coordinate descent, which corresponds to the graphical lasso with zero penalty. For large, sparse graphs, the iterative proportional scaling algorithm appears feasible and has simple convergence properties. The algorithm based on neighbourhood coordinate descent is extremely fast and less dependent on sparsity, but needs a positive-definite starting value to converge. We provide an algorithm for finding such a starting value for graphs with low colouring number. As a consequence, we also obtain a simplified proof of existence of the maximum likelihood estimator in such cases.}",
    issn = {1464-3510},
    doi = {10.1093/biomet/asae028},
    url = {https://doi.org/10.1093/biomet/asae028},
    eprint = {https://academic.oup.com/biomet/advance-article-pdf/doi/10.1093/biomet/asae028/58722495/asae028.pdf},
}

@article {wang.2015,
    AUTHOR = {Wang, Hao},
     TITLE = {Scaling it up: stochastic search structure learning in
              graphical models},
   JOURNAL = {Bayesian Anal.},
  FJOURNAL = {Bayesian Analysis},
    VOLUME = {10},
      YEAR = {2015},
    NUMBER = {2},
     PAGES = {351--377},
      ISSN = {1936-0975},
   MRCLASS = {62F15 (62H12 62H99)},
  MRNUMBER = {3420886},
}

@article {wang.2010,
    AUTHOR = {Wang, Hao},
     TITLE = {Sparse seemingly unrelated regression modelling: applications
              in finance and econometrics},
   JOURNAL = {Comput. Statist. Data Anal.},
  FJOURNAL = {Computational Statistics \& Data Analysis},
    VOLUME = {54},
      YEAR = {2010},
    NUMBER = {11},
     PAGES = {2866--2877},
      ISSN = {0167-9473},
   MRCLASS = {Expansion},
  MRNUMBER = {2720481},
}

@Article{wang.2014,
author={Wang, Hao},
title={Coordinate descent algorithm for covariance graphical lasso},
journal={Statistics and Computing},
year={2014},
month={Jul},
day={01},
volume={24},
number={4},
pages={521-529},
abstract={Bien and Tibshirani (Biometrika, 98(4):807--820, 2011) have proposed a covariance graphical lasso method that applies a lasso penalty on the elements of the covariance matrix. This method is definitely useful because it not only produces sparse and positive definite estimates of the covariance matrix but also discovers marginal independence structures by generating exact zeros in the estimated covariance matrix. However, the objective function is not convex, making the optimization challenging. Bien and Tibshirani (Biometrika, 98(4):807--820, 2011) described a majorize-minimize approach to optimize it. We develop a new optimization method based on coordinate descent. We discuss the convergence property of the algorithm. Through simulation experiments, we show that the new algorithm has a number of advantages over the majorize-minimize approach, including its simplicity, computing speed and numerical stability. Finally, we show that the cyclic version of the coordinate descent algorithm is more efficient than the greedy version.},
issn={1573-1375},
doi={10.1007/s11222-013-9385-5},
url={https://doi.org/10.1007/s11222-013-9385-5}
}

@article{zhang_etal.2022,
author = {Zhang, Jiawei and Yang, Yuhong and Ding, Jie},
title = {Information criteria for model selection},
journal = {WIREs Computational Statistics},
volume = {15},
number = {5},
pages = {e1607},
keywords = {Akaike information criterion, Bayesian information criterion, information criteria, model selection, variable selection},
doi = {https://doi.org/10.1002/wics.1607},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1607},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1607},
abstract = {Abstract The rapid development of modeling techniques has brought many opportunities for data-driven discovery and prediction. However, this also leads to the challenge of selecting the most appropriate model for any particular data task. Information criteria, such as the Akaike information criterion (AIC) and Bayesian information criterion (BIC), have been developed as a general class of model selection methods with profound connections with foundational thoughts in statistics and information theory. Many perspectives and theoretical justifications have been developed to understand when and how to use information criteria, which often depend on particular data circumstances. This review article will revisit information criteria by summarizing their key concepts, evaluation metrics, fundamental properties, interconnections, recent advancements, and common misconceptions to enrich the understanding of model selection in general. This article is categorized under: Data: Types and Structure > Traditional Statistical Data Statistical Learning and Exploratory Methods of the Data Sciences > Modeling Methods Statistical and Graphical Methods of Data Analysis > Information Theoretic Methods Statistical Models > Model Selection},
year = {2023}
}

@book {ando.2010,
    AUTHOR = {Ando, Tomohiro},
     TITLE = {Bayesian model selection and statistical modeling},
    SERIES = {Statistics: Textbooks and Monographs},
 PUBLISHER = {CRC Press, Boca Raton, FL},
      YEAR = {2010},
     PAGES = {xiv+286},
      ISBN = {978-1-4398-3614-9},
   MRCLASS = {62F15 (65C05 65C40 68U20)},
  MRNUMBER = {2722879},
MRREVIEWER = {Kajal\ Lahiri},
       DOI = {10.1201/EBK1439836149},
       URL = {https://doi.org/10.1201/EBK1439836149},
}

@article {burnham_anderson.2004,
    AUTHOR = {Burnham, Kenneth P. and Anderson, David R.},
     TITLE = {Multimodel inference: understanding {AIC} and {BIC} in model
              selection},
   JOURNAL = {Sociol. Methods Res.},
  FJOURNAL = {Sociological Methods \& Research},
    VOLUME = {33},
      YEAR = {2004},
    NUMBER = {2},
     PAGES = {261--304},
      ISSN = {0049-1241,1552-8294},
   MRCLASS = {99-01},
  MRNUMBER = {2086350},
       DOI = {10.1177/0049124104268644},
       URL = {https://doi.org/10.1177/0049124104268644},
}

@book {burnham_anderson.2002,
    AUTHOR = {Burnham, Kenneth P. and Anderson, David R.},
     TITLE = {Model {S}election and {M}ultimodel {I}nference: A {P}ractical {I}nformation-{T}heoretic {A}pproach},
   EDITION = {Second},
 PUBLISHER = {Springer-Verlag, New York},
      YEAR = {2002},
     PAGES = {xxvi+488},
      ISBN = {0-387-95364-7},
   MRCLASS = {99-01},
  MRNUMBER = {1919620},
}

@article {johnstone.2001,
    AUTHOR = {Johnstone, Iain M.},
     TITLE = {On the distribution of the largest eigenvalue in principal
              components analysis},
   JOURNAL = {Ann. Statist.},
  FJOURNAL = {The Annals of Statistics},
    VOLUME = {29},
      YEAR = {2001},
    NUMBER = {2},
     PAGES = {295--327},
      ISSN = {0090-5364,2168-8966},
   MRCLASS = {62H25 (15A52 33C45 33E17 60F05)},
  MRNUMBER = {1863961},
       DOI = {10.1214/aos/1009210544},
       URL = {https://doi.org/10.1214/aos/1009210544},
}

@article {wong_etal.2003,
    AUTHOR = {Wong, Frederick and Carter, Christopher K. and Kohn, Robert},
     TITLE = {Efficient estimation of covariance selection models},
   JOURNAL = {Biometrika},
  FJOURNAL = {Biometrika},
    VOLUME = {90},
      YEAR = {2003},
    NUMBER = {4},
     PAGES = {809--830},
      ISSN = {0006-3444,1464-3510},
   MRCLASS = {62J10 (62F10)},
  MRNUMBER = {2024759},
       DOI = {10.1093/biomet/90.4.809},
       URL = {https://doi.org/10.1093/biomet/90.4.809},
}

@article {smith_kohn.2002,
    AUTHOR = {Smith, Michael and Kohn, Robert},
     TITLE = {Parsimonious covariance matrix estimation for longitudinal
              data},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {97},
      YEAR = {2002},
    NUMBER = {460},
     PAGES = {1141--1153},
      ISSN = {0162-1459,1537-274X},
   MRCLASS = {62J10 (62C10)},
  MRNUMBER = {1951266},
       DOI = {10.1198/016214502388618942},
       URL = {https://doi.org/10.1198/016214502388618942},
}

@article{bertsimas_etal.2020,
author = {Dimitris Bertsimas and Jean Pauphilet and Bart Van Parys},
title = {{Sparse Regression: Scalable Algorithms and Empirical Performance}},
volume = {35},
fjournal = {Statistical Science},
journal={Stat. Sci.},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {555 -- 578},
keywords = {Feature selection},
year = {2020},
doi = {10.1214/19-STS701},
URL = {https://doi.org/10.1214/19-STS701}
}
  
@article{sarwar_etal.2020,
author = {Owais Sarwar and Benjamin Sauk and Nikolaos V. Sahinidis},
title = {{A discussion on practical considerations with sparse regression methodologies}},
volume = {35},
fjournal = {Statistical Science},
journal={Stat. Sci.},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {593 -- 601},
keywords = {Lasso, Sparse regression, subset selection},
year = {2020},
doi = {10.1214/20-STS806},
URL = {https://doi.org/10.1214/20-STS806}
}  

@book {hastie_etal.2015,
    AUTHOR = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
     TITLE = {Statistical {L}earning with {S}parsity. {T}he {L}asso and {G}eneralizations},
    SERIES = {Monographs on Statistics and Applied Probability},
    VOLUME = {143},
 PUBLISHER = {CRC Press, Boca Raton, FL},
      YEAR = {2015},
     PAGES = {xv+351},
      ISBN = {978-1-4987-1216-3},
   MRCLASS = {62-02 (62F15 62G08 62H12 62J05 62J07 62J12 62M15)},
  MRNUMBER = {3616141},
MRREVIEWER = {Su-Yun\ Chen\ Huang},
}

@article {feng_etal.2008,
    AUTHOR = {Liang, Feng and Paulo, Rui and Molina, German and Clyde,
              Merlise A. and Berger, Jim O.},
     TITLE = {Mixtures of {$g$} priors for {B}ayesian variable selection},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {103},
      YEAR = {2008},
    NUMBER = {481},
     PAGES = {410--423},
      ISSN = {0162-1459,1537-274X},
   MRCLASS = {99-01},
  MRNUMBER = {2420243},
       DOI = {10.1198/016214507000001337},
       URL = {https://doi.org/10.1198/016214507000001337},
}

@incollection {zellner.1986,
    AUTHOR = {Zellner, Arnold},
     TITLE = {On assessing prior distributions and {B}ayesian regression
              analysis with {$g$}-prior distributions},
 BOOKTITLE = {Bayesian inference and decision techniques},
    VOLUME = {6},
     PAGES = {233--243},
 PUBLISHER = {North-Holland, Amsterdam},
      YEAR = {1986},
      ISBN = {0-444-87712-6},
   MRCLASS = {62J05 (62F15)},
  MRNUMBER = {881437},
}

@article {kass_wasserman.1995,
    AUTHOR = {Kass, Robert E. and Wasserman, Larry},
     TITLE = {A reference {B}ayesian test for nested hypotheses and its
              relationship to the {S}chwarz criterion},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {90},
      YEAR = {1995},
    NUMBER = {431},
     PAGES = {928--934},
      ISSN = {0162-1459,1537-274X},
   MRCLASS = {62F15},
  MRNUMBER = {1354008},
       URL = {http://links.jstor.org/sici?sici=0162-1459(199509)90:431<928:ARBTFN>2.0.CO;2-B&origin=MSN},
}

@article{chi_lange.2015,
author = {Eric C. Chi and Kenneth Lange},
title = {Splitting methods for convex clustering},
JOURNAL = {J. Comput. Graph. Stat.},
  FJOURNAL = {Journal of Computational and Graphical Statistics},
volume = {24},
number = {4},
pages = {994-1013},
year  = {2015}
}

@article{lindsten_2011,
author = {Lindsten, F. and Ohlsson, H. and Ljung, L.},
title = {Just Relax and Come Clustering! A Convexication of k-Means
Clustering},
journal = {Technical Report, Link\"opings Universitet},
year = {2011}
}

@article{tan_witten_2015,
author = {Kean Ming Tan and Daniela Witten},
title = {{Statistical properties of convex clustering}},
volume = {9},
journal = {Electronic Journal of Statistics},
number = {2},
pages = {2324 -- 2347},
year = {2015}
}

@inproceedings{hocking_etal.2011,
author = {Hocking, Toby Dylan and Joulin, Armand and Bach, Francis and Vert, Jean-Philippe},
title = {Clusterpath: An Algorithm for Clustering Using Convex Fusion Penalties},
year = {2011},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {745–752}
}

@book {hastie_tibshirani.1990,
    AUTHOR = {Hastie, T. J. and Tibshirani, R. J.},
     TITLE = {Generalized additive models},
    SERIES = {Monographs on Statistics and Applied Probability},
    VOLUME = {43},
 PUBLISHER = {Chapman and Hall, Ltd., London},
      YEAR = {1990},
     PAGES = {xvi+335},
      ISBN = {0-412-34390-8},
   MRCLASS = {62J02 (62-07 62G05 62J20)},
  MRNUMBER = {1082147},
MRREVIEWER = {Bent\ J\o rgensen},
}

@article {yen.2011,
    AUTHOR = {Yen, Tso-Jung},
     TITLE = {A majorization-minimization approach to variable selection
              using spike and slab priors},
   JOURNAL = {Ann. Statist.},
  FJOURNAL = {The Annals of Statistics},
    VOLUME = {39},
      YEAR = {2011},
    NUMBER = {3},
     PAGES = {1748--1775},
      ISSN = {0090-5364},
   MRCLASS = {62H12 (62F15 62J05)},
  MRNUMBER = {2850219},
MRREVIEWER = {Marvin H. J. Gruber},
       DOI = {10.1214/11-AOS884},
       URL = {https://doi.org/10.1214/11-AOS884},
}


@article{wood.2011,
author = {Wood, Simon N.},
title = {Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models},
JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
volume = {73},
number = {1},
pages = {3-36},
keywords = {Adaptive smoothing, Generalized additive mixed model, Generalized additive model, Generalized cross-validation, Marginal likelihood, Model selection, Penalized generalized linear model, Penalized regression splines, Restricted maximum likelihood, Scalar on function regression, Stable computation},
doi = {https://doi.org/10.1111/j.1467-9868.2010.00749.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2010.00749.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2010.00749.x},
year = {2011}
}

@inbook{louzada_etal.2023,
author = {Louzada, Francisco and Ferreira, Paulo H. and Nascimento, Diego C.},
publisher = {John Wiley \& Sons, Ltd},
isbn = {9781118445112},
title = {Spike-and-Slab Priors and Their Applications},
booktitle = {Wiley StatsRef: Statistics Reference Online},
chapter = {},
pages = {1-8},
doi = {https://doi.org/10.1002/9781118445112.stat08417},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat08417},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat08417},
year = {2023},
keywords = {Bayesian variable selection, compositional data, high-dimensional regression, LASSO, mixture priors}
}

@book {rue_held.2005,
    AUTHOR = {Rue, H\aa vard and Held, Leonhard},
     TITLE = {Gaussian {M}arkov {R}andom {F}ields},
    SERIES = {Monographs on Statistics and Applied Probability},
    VOLUME = {104},
      NOTE = {Theory and applications},
 PUBLISHER = {Chapman \& Hall/CRC, Boca Raton, FL},
      YEAR = {2005},
     PAGES = {xii+263},
      ISBN = {978-1-58488-432-3; 1-58488-432-0},
   MRCLASS = {60-02 (60G60 62M40)},
  MRNUMBER = {2130347},
MRREVIEWER = {Alexander V. Bulinski\u{\i}},
       DOI = {10.1201/9780203492024},
       URL = {https://doi.org/10.1201/9780203492024},
}


@article {yue_rue.2011,
    AUTHOR = {Yue, Yu Ryan and Rue, H\aa vard},
     TITLE = {Bayesian inference for additive mixed quantile regression
              models},
   JOURNAL = {Comput. Statist. Data Anal.},
  FJOURNAL = {Computational Statistics \& Data Analysis},
    VOLUME = {55},
      YEAR = {2011},
    NUMBER = {1},
     PAGES = {84--96},
      ISSN = {0167-9473},
   MRCLASS = {Expansion},
  MRNUMBER = {2736538},
       DOI = {10.1016/j.csda.2010.05.006},
       URL = {https://doi.org/10.1016/j.csda.2010.05.006},
}


@article {bernardi_etal.2018,
    AUTHOR = {Bernardi, Mauro and Bottone, Marco and Petrella, Lea},
     TITLE = {Bayesian quantile regression using the skew exponential power
              distribution},
   JOURNAL = {Comput. Statist. Data Anal.},
  FJOURNAL = {Computational Statistics \& Data Analysis},
    VOLUME = {126},
      YEAR = {2018},
     PAGES = {92--111},
      ISSN = {0167-9473},
   MRCLASS = {62J99 (62F15)},
  MRNUMBER = {3808392},
       DOI = {10.1016/j.csda.2018.04.008},
       URL = {https://doi.org/10.1016/j.csda.2018.04.008},
}

@article{wahba.1978,
author = {Wahba, Grace},
title = {Improper {P}riors, {S}pline {S}moothing and the {P}roblem of {G}uarding {A}gainst {M}odel {E}rrors in {R}egression},
JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
volume = {40},
number = {3},
pages = {364-372},
keywords = {spline smoothing, improper priors, nonparametric regression, model errors},
doi = {https://doi.org/10.1111/j.2517-6161.1978.tb01050.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1978.tb01050.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1978.tb01050.x},
year = {1978}
}

@article {lindgren_etal.2022,
    AUTHOR = {Lindgren, Finn and Bolin, David and Rue, H\aa vard},
     TITLE = {The {SPDE} approach for {G}aussian and non-{G}aussian fields:
              10 years and still running},
   JOURNAL = {Spat. Stat.},
  FJOURNAL = {Spatial Statistics},
    VOLUME = {50},
      YEAR = {2022},
     PAGES = {Paper No. 100599, 29},
      ISSN = {2211-6753},
   MRCLASS = {99-01},
  MRNUMBER = {4439328},
       DOI = {10.1016/j.spasta.2022.100599},
       URL = {https://doi.org/10.1016/j.spasta.2022.100599},
}

@article {lindgren_etal.2011,
    AUTHOR = {Lindgren, Finn and Rue, H\aa vard and Lindstr\"om, Johan},
     TITLE = {An explicit link between {G}aussian fields and {G}aussian
              {M}arkov random fields: the stochastic partial differential
              equation approach},
      NOTE = {With discussion and a reply by the authors},
   JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
    VOLUME = {73},
      YEAR = {2011},
    NUMBER = {4},
     PAGES = {423--498},
      ISSN = {1369-7412,1467-9868},
   MRCLASS = {62M30 (60H15 62F15 62H99 62M40 62P12)},
  MRNUMBER = {2853727},
MRREVIEWER = {C\'ecile\ Hardouin},
       DOI = {10.1111/j.1467-9868.2011.00777.x},
       URL = {https://doi.org/10.1111/j.1467-9868.2011.00777.x},
}

@article{lang_brezger.2004,
author = {Stefan Lang and Andreas Brezger},
title = {Bayesian P-Splines},
JOURNAL = {J. Comput. Graph. Stat.},
  FJOURNAL = {Journal of Computational and Graphical Statistics},
volume = {13},
number = {1},
pages = {183--212},
year = {2004},
publisher = {ASA Website},
doi = {10.1198/1061860043010},
URL = {https://doi.org/10.1198/1061860043010},
eprint = {https://doi.org/10.1198/1061860043010
}
}

@article{scheipl_kneib.2009,
title = {Locally adaptive Bayesian P-splines with a Normal-Exponential-Gamma prior},
journal = {Computational Statistics \& Data Analysis},
volume = {53},
number = {10},
pages = {3533-3552},
year = {2009},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2009.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167947309001030},
author = {Fabian Scheipl and Thomas Kneib}
}

@book {durbin_koopman.2012,
    AUTHOR = {Durbin, J. and Koopman, S. J.},
     TITLE = {Time {S}eries {A}nalysis by {S}tate {S}pace {M}ethods},
    SERIES = {Oxford Statistical Science Series},
    VOLUME = {38},
   EDITION = {Second},
 PUBLISHER = {Oxford University Press, Oxford},
      YEAR = {2012},
     PAGES = {xxii+346},
      ISBN = {978-0-19-964117-8},
   MRCLASS = {62M10 (62F10 62F15 62M20 93E11)},
  MRNUMBER = {3014996},
       DOI = {10.1093/acprof:oso/9780199641178.001.0001},
       URL = {https://doi.org/10.1093/acprof:oso/9780199641178.001.0001},
}

@Book{harvey.1989,
  author	= {A.C. Harvey},
  title 	= {Forecasting, {S}tructural {T}ime {S}eries {M}odels and the {K}alman {F}ilter},
  publisher	= {Cambridge University Press},
  address	= {Cambridge},
  edition	= {},
  year  	= {1989}
}

@article{bitto_fruhwirth-schnatter.2019,
title = {Achieving shrinkage in a time-varying parameter model framework},
journal = {Journal of Econometrics},
volume = {210},
number = {1},
pages = {75-97},
year = {2019},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2018.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0304407618302070},
author = {Angela Bitto and Sylvia Frühwirth-Schnatter},
keywords = {Bayesian inference, Bayesian Lasso, Double gamma prior, Hierarchical priors, Kalman filter, Log predictive density scores, Normal–gamma prior, Sparsity, State space model}
}

@article{fruhwirth-schnatter_wagner.2010,
title = {Stochastic model specification search for Gaussian and partial non-Gaussian state space models},
journal = {Journal of Econometrics},
volume = {154},
number = {1},
pages = {85-100},
year = {2010},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2009.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0304407609001614},
author = {Sylvia Frühwirth-Schnatter and Helga Wagner},
keywords = {Auxiliary mixture sampling, Bayesian econometrics, Non-centered parameterization, Markov chain Monte Carlo, Variable selection}
}

@article{huber_etal.2021,
author = {Florian Huber, Gary Koop and Luca Onorante},
title = {Inducing Sparsity and Shrinkage in Time-Varying Parameter Models},
journal = {Journal of Business \& Economic Statistics},
volume = {39},
number = {3},
pages = {669--683},
year = {2021},
publisher = {ASA Website},
doi = {10.1080/07350015.2020.1713796},
URL = {https://doi.org/10.1080/07350015.2020.1713796},
eprint = {https://doi.org/10.1080/07350015.2020.1713796}
}

@book {west_harrison.1997,
    AUTHOR = {West, Mike and Harrison, Jeff},
     TITLE = {Bayesian forecasting and dynamic models},
    SERIES = {Springer Series in Statistics},
   EDITION = {Second},
 PUBLISHER = {Springer-Verlag, New York},
      YEAR = {1997},
     PAGES = {xiv+680},
      ISBN = {0-387-94725-6},
   MRCLASS = {62M20 (62F15 90A20)},
  MRNUMBER = {1482232},
}

@article{ko_etal.2022,
author = {Seyoon Ko and Hua Zhou and Jin J. Zhou and Joong-Ho Won},
title = {{High-performance statistical computing in the computing environments of the 2020s}},
volume = {37},
fjournal = {Statistical Science},
journal={Stat. Sci.},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {494 -- 518},
keywords = {ADMM, Cloud computing, Cox regression, deep learning, graphics processing units (GPUs), High-performance statistical computing, MM algorithms, PDHG},
year = {2022},
doi = {10.1214/21-STS835},
URL = {https://doi.org/10.1214/21-STS835}
}

@article{chan_jeliazkov.2009,
author = {Chan, Joshua C.C. and Jeliazkov, Ivan},
title = {Efficient simulation and integrated likelihood estimation in state space models},
journal = {International Journal of Mathematical Modelling and Numerical Optimisation},
volume = {1},
number = {1-2},
pages = {101-120},
year = {2009},
doi = {10.1504/IJMMNO.2009.03009},
URL = {https://www.inderscienceonline.com/doi/abs/10.1504/IJMMNO.2009.03009},
eprint = {https://www.inderscienceonline.com/doi/pdf/10.1504/IJMMNO.2009.03009}
}

@article {fahrmeir_etal.2004,
    AUTHOR = {Fahrmeir, Ludwig and Kneib, Thomas and Lang, Stefan},
     TITLE = {Penalized structured additive regression for space-time data:
              a {B}ayesian perspective},
   JOURNAL = {Statist. Sinica},
  FJOURNAL = {Statistica Sinica},
    VOLUME = {14},
      YEAR = {2004},
    NUMBER = {3},
     PAGES = {731--761},
      ISSN = {1017-0405},
   MRCLASS = {62G05 (62C10 62C12 62J10 62J12)},
  MRNUMBER = {2087971},
}

@article {fahrmeir_kaufmann.1991,
    AUTHOR = {Fahrmeir, L. and Kaufmann, H.},
     TITLE = {On {K}alman filtering, posterior mode estimation and {F}isher
              scoring in dynamic exponential family regression},
   JOURNAL = {Metrika},
  FJOURNAL = {Metrika. International Journal for Theoretical and Applied
              Statistics},
    VOLUME = {38},
      YEAR = {1991},
    NUMBER = {1},
     PAGES = {37--60},
      ISSN = {0026-1335},
   MRCLASS = {62M20 (62M10)},
  MRNUMBER = {1093375},
MRREVIEWER = {Bent J\o rgensen},
       DOI = {10.1007/BF02613597},
       URL = {https://doi.org/10.1007/BF02613597},
}

@book {wahba.1990,
    AUTHOR = {Wahba, Grace},
     TITLE = {Spline models for observational data},
    SERIES = {CBMS-NSF Regional Conference Series in Applied Mathematics},
    VOLUME = {59},
 PUBLISHER = {Society for Industrial and Applied Mathematics (SIAM),
              Philadelphia, PA},
      YEAR = {1990},
     PAGES = {xii+169},
      ISBN = {0-89871-244-0},
   MRCLASS = {62G05 (62J02 65D10 65U05)},
  MRNUMBER = {1045442},
MRREVIEWER = {Girdhar G. Agarwal},
       DOI = {10.1137/1.9781611970128},
       URL = {https://doi.org/10.1137/1.9781611970128},
}

@article {eubank_etal.2004,
    AUTHOR = {Eubank, R. L. and Huang, Chunfeng and Mu\~{n}oz Maldonado, Y. and
              Wang, Naisyin and Wang, Suojin and Buchanan, R. J.},
     TITLE = {Smoothing spline estimation in varying-coefficient models},
   JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
    VOLUME = {66},
      YEAR = {2004},
    NUMBER = {3},
     PAGES = {653--667},
      ISSN = {1369-7412},
   MRCLASS = {62G05},
  MRNUMBER = {2088294},
       DOI = {10.1111/j.1467-9868.2004.B5595.x},
       URL = {https://doi.org/10.1111/j.1467-9868.2004.B5595.x},
}

@article{scheipl_etal.2012,
	author = {Fabian   Scheipl  and  Ludwig   Fahrmeir  and  Thomas   Kneib },
	title = {Spike-and-slab priors for function selection in structured additive regression models},
	   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
	volume = {107},
	number = {500},
	pages = {1518-1532},
	year  = {2012},
	publisher = {Taylor & Francis},
	doi = {10.1080/01621459.2012.737742},
	URL = {https://doi.org/10.1080/01621459.2012.737742},
	eprint = {https://doi.org/10.1080/01621459.2012.737742}
}

@book{seber_lee.2012,
  title={Linear regression analysis},
  author={Seber, George AF and Lee, Alan J},
  year={2012},
  publisher={John Wiley \& Sons}
}

@article{hill_etal.2020,
   author = "Hill, Jennifer and Linero, Antonio and Murray, Jared",
   title = "Bayesian additive regression trees: a review and look forward", 
   fjournal= "Annual Review of Statistics and Its Application",
   journal="Annu. Rev. Stat. Its Appl.",
   year = "2020",
   volume = "7",
   number = "Volume 7, 2020",
   pages = "251-278",
   doi = "https://doi.org/10.1146/annurev-statistics-031219-041110",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031219-041110",
   publisher = "Annual Reviews",
   issn = "2326-831X",
   type = "Journal Article",
   keywords = "Bayesian nonparametrics",
   keywords = "regression",
   keywords = "regularization",
   keywords = "machine learning",
   keywords = "causal inference"
  }

@article{bottegal_pillonetto.2018,
title = {The generalized cross validation filter},
journal = {Automatica},
volume = {90},
pages = {130-137},
year = {2018},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2017.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S0005109817306416},
author = {Giulio Bottegal and Gianluigi Pillonetto},
keywords = {Kalman filtering, Generalized cross-validation, On-line system identification, Inverse problems, Regularization, Smoothness parameter, Splines},
abstract = {Generalized cross validation (GCV) is one of the most important approaches used to estimate parameters in the context of inverse problems and regularization techniques. A notable example is the determination of the smoothness parameter in splines. When the data are generated by a state space model, like in the spline case, efficient algorithms are available to evaluate the GCV score with complexity that scales linearly in the data set size. However, these methods are not amenable to on-line applications since they rely on forward and backward recursions. Hence, if the objective has been evaluated at time t−1 and new data arrive at time t, then O(t) operations are needed to update the GCV score. In this paper we instead show that the update cost isO(1), thus paving the way to the on-line use of GCV. This result is obtained by deriving the novel GCV filter which extends the classical Kalman filter equations to efficiently propagate the GCV score over time. We also illustrate applications of the new filter in the context of state estimation and on-line regularized linear system identification.}
}

@article{ansley_kohn.1987,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2336028},
 abstract = {For a signal plus noise model with a state space representation, an efficient procedure is given for obtaining the trace of the influence matrix, where the influence matrix expresses the estimated signal vector as a linear combination of the observed data. This allows an O(n) evaluation of the generalized cross-validation criterion function. Our approach is very efficient, requiring the addition of only one equation to the ordinary Kalman filter, and extends to models with linear regressors in the observation equation. Important applications are to spline smoothing in regression and to variance components models for seasonal time series.},
 author = {Craig F. Ansley and Robert Kohn},
 journal = {Biometrika},
 number = {1},
 pages = {139--148},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Efficient generalized cross-validation for state space models},
 urldate = {2024-09-27},
 volume = {74},
 year = {1987}
}

@article{kohn_ansley.1989,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2336370},
 abstract = {A fast algorithm is developed for computing the conditional mean and variance of the signal given the observations in a signal plus noise model. The resulting recursions can be applied immediately to provide new and efficient formulae for smoothing part or all of the state vector. The ideas of studentized residuals and leverage from regression analysis are generalized to state space models, and the new algorithm is used to compute the various measures. The results are also applied to obtain a new efficient algorithm for polynomial spline smoothing.},
 author = {Robert Kohn and Craig F. Ansley},
 journal = {Biometrika},
 number = {1},
 pages = {65--79},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {A fast algorithm for signal extraction, influence and cross-validation in state space models},
 urldate = {2024-09-27},
 volume = {76},
 year = {1989}
}

@article{golub_etal.1979,
author = {Gene H. Golub, Michael Heath and Grace Wahba},
title = {Generalized cross-validation as a method for choosing a good ridge parameter},
journal = {Technometrics},
volume = {21},
number = {2},
pages = {215--223},
year = {1979},
publisher = {ASA Website},
doi = {10.1080/00401706.1979.10489751},
URL = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1979.10489751},
eprint = {https://www.tandfonline.com/doi/pdf/10.1080/00401706.1979.10489751
}
}

@book{wang.2011,
  title={Smoothing Splines: Methods and Applications},
  author={Wang, Y.},
  isbn={9781420077568},
  lccn={2011026781},
  series={ISSN},
  url={https://books.google.it/books?id=NH5Spn0yru4C},
  year={2011},
  publisher={CRC Press}
}

@article{luo_song.2020,
author = {Luo, Lan and Song, Peter X.-K.},
title = {Renewable estimation and incremental inference in generalized linear models with streaming data sets},
JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
volume = {82},
number = {1},
pages = {69-97},
keywords = {Incremental statistical analysis, Lambda architecture, On-line learning, Spark computing platform, Stochastic gradient descent algorithm},
doi = {https://doi.org/10.1111/rssb.12352},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12352},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12352},
abstract = {Summary The paper presents an incremental updating algorithm to analyse streaming data sets using generalized linear models. The method proposed is formulated within a new framework of renewable estimation and incremental inference, in which the maximum likelihood estimator is renewed with current data and summary statistics of historical data. Our framework can be implemented within a popular distributed computing environment, known as Apache Spark, to scale up computation. Consisting of two data-processing layers, the rho architecture enables us to accommodate inference-related statistics and to facilitate sequential updating of the statistics used in both estimation and inference. We establish estimation consistency and asymptotic normality of the proposed renewable estimator, in which the Wald test is utilized for an incremental inference. Our methods are examined and illustrated by various numerical examples from both simulation experiments and a real world data analysis.},
year = {2020}
}

@Article{lu_etal.2015,
author={Lu, Haibo
and Wang, Zhanfeng
and Wu, Yaohua},
title={Sequential estimate for generalized linear models with uncertain number of effective variables},
fjournal={Journal of Systems Science and Complexity},
journal={J. Syst. Sci. Complex},
year={2015},
month={Apr},
day={01},
volume={28},
number={2},
pages={424-438},
abstract={For the generalized linear model, the authors propose a sequential sampling procedure based on an adaptive shrinkage estimate of parameter. This method can determine a minimum sample size under which effective variables contributing to the model are identified and estimates of regression parameters achieve the required accuracy. The authors prove that the proposed sequential procedure is asymptotically optimal. Numerical simulation studies show that the proposed method can save a large number of samples compared to the traditional sequential approach.},
issn={1559-7067},
doi={10.1007/s11424-015-3110-8},
url={https://doi.org/10.1007/s11424-015-3110-8}
}

@article{van_wieringen_binder.2022,
author = {Wessel N. van Wieringen and Harald Binder},
title = {Sequential learning of regression models by penalized estimation},
JOURNAL = {J. Comput. Graph. Stat.},
  FJOURNAL = {Journal of Computational and Graphical Statistics},
volume = {31},
number = {3},
pages = {877--886},
year = {2022},
publisher = {ASA Website},
doi = {10.1080/10618600.2022.2035231},
URL = {https://doi.org/10.1080/10618600.2022.2035231},
eprint = {https://doi.org/10.1080/10618600.2022.2035231}
}

@BOOK{shalev-shwartz.2012,
  author={Shalev-Shwartz, Shai},
  booktitle={Online Learning and Online Convex Optimization},
  year={2012},
  volume={},
  number={},
  pages={},
  keywords={Artificial Intelligence;Machine Learning;Computer Science},
  doi={10.1561/2200000018}}

@article{losing_etal.2018,
title = {Incremental on-line learning: A review and comparison of state of the art algorithms},
journal = {Neurocomputing},
volume = {275},
pages = {1261-1274},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.06.084},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217315928},
author = {Viktor Losing and Barbara Hammer and Heiko Wersing},
keywords = {Incremental learning, On-line learning, Data streams, Hyperparameter optimization, Model selection},
abstract = {Recently, incremental and on-line learning gained more attention especially in the context of big data and learning from data streams, conflicting with the traditional assumption of complete data availability. Even though a variety of different methods are available, it often remains unclear which of them is suitable for a specific task and how they perform in comparison to each other. We analyze the key properties of eight popular incremental methods representing different algorithm classes. Thereby, we evaluate them with regards to their on-line classification error as well as to their behavior in the limit. Further, we discuss the often neglected issue of hyperparameter optimization specifically for each method and test how robustly it can be done based on a small set of examples. Our extensive evaluation on data sets with different characteristics gives an overview of the performance with respect to accuracy, convergence speed as well as model complexity, facilitating the choice of the best method for a given application.}
}

@book {konishi_kitagawa.2008,
    AUTHOR = {Konishi, Sadanori and Kitagawa, Genshiro},
     TITLE = {Information criteria and statistical modeling},
    SERIES = {Springer Series in Statistics},
 PUBLISHER = {Springer, New York},
      YEAR = {2008},
     PAGES = {xii+273},
      ISBN = {978-0-387-71886-6},
   MRCLASS = {62-02 (62B10 94A15)},
  MRNUMBER = {2367855},
       DOI = {10.1007/978-0-387-71887-3},
       URL = {https://doi.org/10.1007/978-0-387-71887-3},
}

@article {jenatton_etal.2011,
    AUTHOR = {Jenatton, Rodolphe and Audibert, Jean-Yves and Bach, Francis},
     TITLE = {Structured variable selection with sparsity-inducing norms},
   JOURNAL = {J. Mach. Learn. Res.},
  FJOURNAL = {Journal of Machine Learning Research (JMLR)},
    VOLUME = {12},
      YEAR = {2011},
     PAGES = {2777--2824},
      ISSN = {1532-4435,1533-7928},
   MRCLASS = {62J07 (62F07 68T05 90C25)},
  MRNUMBER = {2854347},
MRREVIEWER = {Xiaogang\ Su},
}

@book {hastie_etal.2009,
    AUTHOR = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
     TITLE = {The {E}lements of {S}tatistical {L}earning. Data mining, inference, and prediction},
   EDITION = {Second},
 PUBLISHER = {Springer, New York},
      YEAR = {2009},
     PAGES = {xxii+745},
      ISBN = {978-0-387-84857-0},
   MRCLASS = {62Gxx (62-09 62H25 62H30 62J07 68T05)},
  MRNUMBER = {2722294},
MRREVIEWER = {Gilles\ Blanchard},
       DOI = {10.1007/978-0-387-84858-7},
       URL = {https://doi.org/10.1007/978-0-387-84858-7},
}

@book {kontoghiorghes.2006,
     TITLE = {Handbook of {P}arallel {C}omputing and {S}tatistics},
    SERIES = {Statistics: Textbooks and Monographs},
    VOLUME = {184},
    EDITOR = {Kontoghiorghes, Erricos John},
 PUBLISHER = {Chapman \& Hall/CRC, Boca Raton, FL},
      YEAR = {2006},
     PAGES = {xvi+530},
      ISBN = {978-0-8247-4067-2; 0-8247-4067-X},
   MRCLASS = {68-00 (62-06 65Y05 68W10)},
  MRNUMBER = {2265409},
}

@Article{adams_etal.1996,
author={Adams, N. M.
and Kirby, S. P. J.
and Harris, P.
and Clegg, D. B.},
title={A review of parallel processing for statistical computation},
journal={Statistics and Computing},
year={1996},
month={Mar},
day={01},
volume={6},
number={1},
pages={37-49},
abstract={Parallel computers differ from conventional serial computers in that they can, in a variety of ways, perform more than one operation at a time. Parallel processing, the application of parallel computers, has been successfully utilized in many fields of science and technology. The purpose of this paper is to review efforts to use parallel processing for statistical computing. We present some technical background, followed by a review of the literature that relates parallel computing to statistics. The review material focuses explicitly on statistical methods and applications, rather than on conventional mathematical techniques. Thus, most of the review material is drawn from statistics publications. We conclude by discussing the nature of the review material and considering some possibilities for the future.},
issn={1573-1375},
doi={10.1007/BF00161572},
url={https://doi.org/10.1007/BF00161572}
}

@book {lin_etal.2022,
    AUTHOR = {Lin, Zhouchen and Li, Huan and Fang, Cong},
     TITLE = {Alternating {D}irection {M}ethod of {M}ultipliers for {M}achine
              {L}earning},
 PUBLISHER = {Springer, Singapore},
      YEAR = {[2022] \copyright 2022},
     PAGES = {xxiii+263},
      ISBN = {978-981-16-9839-2; 978-981-16-9840-8},
   MRCLASS = {90-02 (62M45 68T05 90C25 90C26)},
  MRNUMBER = {4461110},
       DOI = {10.1007/978-981-16-9840-8},
       URL = {https://doi.org/10.1007/978-981-16-9840-8},
}

@article {xue_etal.2012,
    AUTHOR = {Xue, Lingzhou and Ma, Shiqian and Zou, Hui},
     TITLE = {Positive-definite {$\ell_1$}-penalized estimation of large
              covariance matrices},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
    VOLUME = {107},
      YEAR = {2012},
    NUMBER = {500},
     PAGES = {1480--1491},
      ISSN = {0162-1459,1537-274X},
   MRCLASS = {62H12 (62J07)},
  MRNUMBER = {3036409},
MRREVIEWER = {Marvin\ H. J. Gruber},
       DOI = {10.1080/01621459.2012.725386},
       URL = {https://doi.org/10.1080/01621459.2012.725386},
}

@article {ramdas_tibshirani.2016,
    AUTHOR = {Ramdas, Aaditya and Tibshirani, Ryan J.},
     TITLE = {Fast and flexible {ADMM} algorithms for trend filtering},
   JOURNAL = {J. Comput. Graph. Statist.},
  FJOURNAL = {Journal of Computational and Graphical Statistics},
    VOLUME = {25},
      YEAR = {2016},
    NUMBER = {3},
     PAGES = {839--858},
      ISSN = {1061-8600,1537-2715},
   MRCLASS = {62G08 (62-04)},
  MRNUMBER = {3533641},
       DOI = {10.1080/10618600.2015.1054033},
       URL = {https://doi.org/10.1080/10618600.2015.1054033},
}

@article {zhu.2017,
    AUTHOR = {Zhu, Yunzhang},
     TITLE = {An augmented {ADMM} algorithm with application to the
              generalized lasso problem},
   JOURNAL = {J. Comput. Graph. Statist.},
  FJOURNAL = {Journal of Computational and Graphical Statistics},
    VOLUME = {26},
      YEAR = {2017},
    NUMBER = {1},
     PAGES = {195--204},
      ISSN = {1061-8600,1537-2715},
   MRCLASS = {62J07},
  MRNUMBER = {3610420},
       DOI = {10.1080/10618600.2015.1114491},
       URL = {https://doi.org/10.1080/10618600.2015.1114491},
}

@article {lee_etal.2017,
    AUTHOR = {Lee, Taehoon and Won, Joong-Ho and Lim, Johan and Yoon,
              Sungroh},
     TITLE = {Large-scale structured sparsity via parallel fused lasso on
              multiple {GPU}s},
   JOURNAL = {J. Comput. Graph. Statist.},
  FJOURNAL = {Journal of Computational and Graphical Statistics},
    VOLUME = {26},
      YEAR = {2017},
    NUMBER = {4},
     PAGES = {851--864},
      ISSN = {1061-8600,1537-2715},
   MRCLASS = {99-01},
  MRNUMBER = {3765349},
       DOI = {10.1080/10618600.2017.1328363},
       URL = {https://doi.org/10.1080/10618600.2017.1328363},
}

@article {gu_etal.2018,
    AUTHOR = {Gu, Yuwen and Fan, Jun and Kong, Lingchen and Ma, Shiqian and
              Zou, Hui},
     TITLE = {A{DMM} for high-dimensional sparse penalized quantile
              regression},
   JOURNAL = {Technometrics},
  FJOURNAL = {Technometrics. A Journal of Statistics for the Physical,
              Chemical and Engineering Sciences},
    VOLUME = {60},
      YEAR = {2018},
    NUMBER = {3},
     PAGES = {319--331},
      ISSN = {0040-1706,1537-2723},
   MRCLASS = {62J07 (62J99)},
  MRNUMBER = {3847169},
       DOI = {10.1080/00401706.2017.1345703},
       URL = {https://doi.org/10.1080/00401706.2017.1345703},
}

@book{saleh.2019,
  title={Theory of ridge regression estimation with applications},
  author={Saleh, AK Md Ehsanes and Arashi, Mohammad and Kibria, BM Golam},
  year={2019},
  publisher={John Wiley \& Sons}
}

@book {lauritzen.1996,
    AUTHOR = {Lauritzen, Steffen L.},
     TITLE = {Graphical models},
    SERIES = {Oxford Statistical Science Series},
    VOLUME = {17},
      NOTE = {Oxford Science Publications},
 PUBLISHER = {The Clarendon Press, Oxford University Press, New York},
      YEAR = {1996},
     PAGES = {x+298},
      ISBN = {0-19-852219-3},
   MRCLASS = {62-01 (05C90 60J99 62H05 62H10 62H17 68T30)},
  MRNUMBER = {1419991},
MRREVIEWER = {M.\ Studen\'y},
}

@article{friedman_etal.2007,
    author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
    title = "{Sparse inverse covariance estimation with the graphical lasso}",
    journal = {Biostatistics},
    volume = {9},
    number = {3},
    pages = {432-441},
    year = {2007},
    month = {12},
    issn = {1465-4644},
    doi = {10.1093/biostatistics/kxm045},
    url = {https://doi.org/10.1093/biostatistics/kxm045},
    eprint = {https://academic.oup.com/biostatistics/article-pdf/9/3/432/57071116/biostatistics\_9\_3\_432.pdf},
}

@article {lu.2009,
    AUTHOR = {Lu, Zhaosong},
     TITLE = {Adaptive first-order methods for general sparse inverse
              covariance selection},
   JOURNAL = {SIAM J. Matrix Anal. Appl.},
  FJOURNAL = {SIAM Journal on Matrix Analysis and Applications},
    VOLUME = {31},
      YEAR = {2009/10},
    NUMBER = {4},
     PAGES = {2000--2016},
      ISSN = {0895-4798,1095-7162},
   MRCLASS = {62H12 (62J10 65C60 90C22 90C25 90C47)},
  MRNUMBER = {2678953},
       DOI = {10.1137/080742531},
       URL = {https://doi.org/10.1137/080742531},
}

@article{anderson_etal.1992,
title = {Generalized QR factorization and its applications},
journal = {Linear Algebra and its Applications},
volume = {162-164},
pages = {243-271},
year = {1992},
issn = {0024-3795},
doi = {https://doi.org/10.1016/0024-3795(92)90379-O},
url = {https://www.sciencedirect.com/science/article/pii/002437959290379O},
author = {E. Anderson and Z. Bai and J. Dongarra},
}

@article{jolliffe_cadima.2016,
author = {Jolliffe, Ian T.  and Cadima, Jorge},
title = {Principal component analysis: a review and recent developments},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
volume = {374},
number = {2065},
pages = {20150202},
year = {2016},
doi = {10.1098/rsta.2015.0202},
URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2015.0202},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2015.0202}
}

@book {bishop.2006,
    AUTHOR = {Bishop, Christopher M.},
     TITLE = {Pattern recognition and machine learning},
    SERIES = {Information Science and Statistics},
 PUBLISHER = {Springer, New York},
      YEAR = {2006},
     PAGES = {xx+738},
      ISBN = {978-0387-31073-2; 0-387-31073-8},
   MRCLASS = {62-01 (62H30 62J05 62M45 68-01 68T05 68T10)},
  MRNUMBER = {2247587},
}

@book{van_loan.1997,
  title={Introduction to Scientific Computing: A Matrix-vector Approach Using MATLAB{\textregistered}},
  author={Charles F. Van Loan},
  year={1997},
  publisher={Prentice Hall.}
}

@book{holmes.2023,
  title={Introduction to {S}cientific {C}omputing and {D}ata {A}nalysis},
  author={Holmes, Mark H},
  volume={13},
  year={2023},
  publisher={Springer Nature}
}

@article{hoerl.2020,
author = {Roger W. Hoerl},
title = {Ridge Regression: A Historical Context},
journal = {Technometrics},
volume = {62},
number = {4},
pages = {420--425},
year = {2020},
publisher = {ASA Website},
doi = {10.1080/00401706.2020.1742207},
URL = {https://doi.org/10.1080/00401706.2020.1742207},
eprint = { https://doi.org/10.1080/00401706.2020.1742207
}
}

@book{saleh_etal.2019,
  title={Theory of ridge regression estimation with applications},
  author={Saleh, AK Md Ehsanes and Arashi, Mohammad and Kibria, BM Golam},
  year={2019},
  publisher={John Wiley \& Sons}
}

@book{gruber.1998,
  title={Improving efficiency by shrinkage: The James--Stein and Ridge regression estimators},
  author={Gruber, Marvin},
  year={1998},
  publisher={Routledge}
}

@article{wood_etal.2015,
    author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
    title = "{Generalized additive models for large data sets}",
    fjournal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
    JOURNAL = {J. R. Stat. Soc. Ser. C Appl. Stat.},
    volume = {64},
    number = {1},
    pages = {139-155},
    year = {2014},
    month = {05},
    issn = {0035-9254},
    doi = {10.1111/rssc.12068},
    url = {https://doi.org/10.1111/rssc.12068},
    eprint = {https://academic.oup.com/jrsssc/article-pdf/64/1/139/49402374/jrsssc\_64\_1\_139.pdf},
}

@article{wood_etal.2016,
author = {Simon N. Wood, Natalya Pya and Benjamin Säfken},
title = {Smoothing parameter and model selection for general smooth models},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
volume = {111},
number = {516},
pages = {1548--1563},
year = {2016},
publisher = {ASA Website},
doi = {10.1080/01621459.2016.1180986},
URL = {https://doi.org/10.1080/01621459.2016.1180986},
eprint = { https://doi.org/10.1080/01621459.2016.1180986}
}

@article {beck_teboulle.2009,
    AUTHOR = {Beck, Amir and Teboulle, Marc},
     TITLE = {A fast iterative shrinkage-thresholding algorithm for linear
              inverse problems},
   JOURNAL = {SIAM J. Imaging Sci.},
  FJOURNAL = {SIAM Journal on Imaging Sciences},
    VOLUME = {2},
      YEAR = {2009},
    NUMBER = {1},
     PAGES = {183--202},
      ISSN = {1936-4954},
   MRCLASS = {35R30 (65F22 90C06 90C25)},
  MRNUMBER = {2486527},
       DOI = {10.1137/080716542},
       URL = {https://doi.org/10.1137/080716542},
}

@article {liang_etal.2022,
    AUTHOR = {Liang, Jingwei and Luo, Tao and Sch\"onlieb, Carola-Bibiane},
     TITLE = {Improving ``fast iterative shrinkage-thresholding algorithm'':
              faster, smarter, and greedier},
   JOURNAL = {SIAM J. Sci. Comput.},
  FJOURNAL = {SIAM Journal on Scientific Computing},
    VOLUME = {44},
      YEAR = {2022},
    NUMBER = {3},
     PAGES = {A1069--A1091},
      ISSN = {1064-8275,1095-7197},
   MRCLASS = {65K05 (65K10 90C25 90C31)},
  MRNUMBER = {4417003},
MRREVIEWER = {Yan\ Tang},
       DOI = {10.1137/21M1395685},
       URL = {https://doi.org/10.1137/21M1395685},
}

@book{bjorck.1996,
author = {Bj\"{o}rck, {\AA}ke},
title = {Numerical Methods for Least Squares Problems},
publisher = {Society for Industrial and Applied Mathematics},
year = {1996},
doi = {10.1137/1.9781611971484},
address = {},
edition   = {},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611971484},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611971484}
}

@book{bjorck.2024,
author = {Bj\"{o}rck, {\AA}ke},
title = {Numerical Methods for Least Squares Problems: Second Edition},
publisher = {Society for Industrial and Applied Mathematics},
year = {2024},
doi = {10.1137/1.9781611977950},
address = {Philadelphia, PA},
edition   = {},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977950},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977950}
}

@book {golub_meurant.2010,
    AUTHOR = {Golub, Gene H. and Meurant, G\'erard},
     TITLE = {Matrices, moments and quadrature with applications},
    SERIES = {Princeton Series in Applied Mathematics},
 PUBLISHER = {Princeton University Press, Princeton, NJ},
      YEAR = {2010},
     PAGES = {xii+363},
      ISBN = {978-0-691-14341-5},
   MRCLASS = {65-02 (65D30)},
  MRNUMBER = {2582949},
MRREVIEWER = {G.\ A.\ Evans},
}

@article{tracy.2022,
  title={A square-root {K}alman filter using only {QR} decompositions},
  author={Tracy, Kevin},
  journal={arXiv preprint arXiv:2208.06452},
  year={2022}
}

@book {monahan.2001,
    AUTHOR = {Monahan, John F.},
     TITLE = {Numerical {M}ethods of {S}tatistics},
    SERIES = {Cambridge Series in Statistical and Probabilistic Mathematics},
    VOLUME = {7},
      PUBLISHER = {Cambridge University Press, Cambridge},
      YEAR = {2001},
     PAGES = {xiv+428},
      ISBN = {0-521-79168-5},
   MRCLASS = {65-01 (60-04 62-04 65C60)},
  MRNUMBER = {1813549},
MRREVIEWER = {Beatrix\ Jones},
       DOI = {10.1017/CBO9780511812231},
       URL = {https://doi.org/10.1017/CBO9780511812231},
}

@book {sarkka_svensson.2023,
    AUTHOR = {S\"arkk\"a, Simo and Svensson, Lennart},
     TITLE = {Bayesian filtering and smoothing},
    SERIES = {Institute of Mathematical Statistics Textbooks},
    VOLUME = {17},
   EDITION = {Second},
 PUBLISHER = {Cambridge University Press, Cambridge},
      YEAR = {2023},
     PAGES = {xxx+406},
      ISBN = {978-1-108-92664-5},
   MRCLASS = {62-01 (62F15 62M15 62M20)},
  MRNUMBER = {4692992},
       DOI = {10.1017/9781108917407},
       URL = {https://doi.org/10.1017/9781108917407},
}

@article {andrew_dingle.2014,
    AUTHOR = {Andrew, Robert and Dingle, Nicholas},
     TITLE = {Implementing {QR} factorization updating algorithms on {GPU}s},
   JOURNAL = {Parallel Comput.},
  FJOURNAL = {Parallel Computing. Systems \& Applications},
    VOLUME = {40},
      YEAR = {2014},
    NUMBER = {7},
     PAGES = {161--172},
      ISSN = {0167-8191,1872-7336},
   MRCLASS = {65F25 (65Y10)},
  MRNUMBER = {3225337},
       DOI = {10.1016/j.parco.2014.03.003},
       URL = {https://doi.org/10.1016/j.parco.2014.03.003},
}

@article{fan_lv.2008,
    author = {Fan, Jianqing and Lv, Jinchi},
    title = "{Sure Independence Screening for Ultrahigh Dimensional Feature Space}",
    JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
    volume = {70},
    number = {5},
    pages = {849-911},
    year = {2008},
    month = {10},
    issn = {1369-7412},
    doi = {10.1111/j.1467-9868.2008.00674.x},
    url = {https://doi.org/10.1111/j.1467-9868.2008.00674.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/70/5/849/49796154/jrsssb\_70\_5\_849.pdf},
}

@article{arlot_celisse.2010,
author = {Sylvain Arlot and Alain Celisse},
title = {{A survey of cross-validation procedures for model selection}},
volume = {4},
journal = {Statistics Surveys},
number = {none},
publisher = {Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
pages = {40 -- 79},
keywords = {cross-validation, leave-one-out, Model selection},
year = {2010},
doi = {10.1214/09-SS054},
URL = {https://doi.org/10.1214/09-SS054}
}

@article{stone.1974,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984809},
 abstract = {A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
 author = {M. Stone},
JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
 number = {2},
 pages = {111--147},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {Cross-Validatory Choice and Assessment of Statistical Predictions},
 urldate = {2024-10-05},
 volume = {36},
 year = {1974}
}

@book{yale_etal.2017,
  title={Handbook of Statistical Analysis and Data Mining Applications},
  author={Yale, Ken and Nisbet, Robert and Miner, Gary D},
  year={2017},
  publisher={Elsevier}
}

@incollection{nisbet_etal.2018,
title = {Feature {S}election},
booktitle = {Handbook of Statistical Analysis and Data Mining Applications},
publisher = {Academic Press},
edition = {Second Edition},
address = {Boston},
pages = {83-97},
year = {2018},
isbn = {978-0-12-416632-5},
doi = {https://doi.org/10.1016/B978-0-12-416632-5.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124166325000050},
author = {Robert Nisbet and Gary Miner and Ken Yale},
keywords = {Variables as features, Types of variable selections, Feature ranking methods, Subset selection methods, Feature selection}
}

@article{elden.1972,
  title={Stepwise regression analysis with orthogonal transformations},
  author={Elden, Lars},
  journal = {Technical Report LiTH-MAT-R-1972-2, Department of Mathematics, Link\"{o}ping University, Sweden},
  year={1972}
}

@article{yanev_kontoghiorghes.2004,
title = {Efficient algorithms for block downdating of least squares solutions},
journal = {Applied Numerical Mathematics},
volume = {49},
number = {1},
pages = {3-15},
year = {2004},
issn = {0168-9274},
doi = {https://doi.org/10.1016/j.apnum.2003.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0168927403001922},
author = {Petko Yanev and Erricos J. Kontoghiorghes},
keywords = {Least squares, QR decomposition, Givens rotations, Householder transformations, Downdating},
abstract = {Five computationally efficient algorithms for block downdating of the least squares solutions are proposed. The algorithms are block versions of Givens rotations strategies and are rich in BLAS-3 operations. They efficiently exploit the triangular structure of the matrices. The theoretical complexities of the algorithms are derived and analyzed. The performance of the implementations confirms the theoretical results. The new strategies are found to outperform existing downdating methods.}
}

@Article{olszanskyj.1994,
author={Olszanskyj, Serge J.
and Lebak, James M.
and Bojanczyk, Adam W.},
title={Rank-k modification methods for recursive least squares problems},
journal={Numerical Algorithms},
year={1994},
month={Sep},
day={01},
volume={7},
number={2},
pages={325-354},
abstract={In least squares problems, it is often desired to solve the same problem repeatedly but with several rows of the data either added, deleted, or both. Methods for quickly solving a problem after adding or deleting one row of data at a time are known. In this paper we introduce fundamental rank-k updating and downdating methods and show how extensions of rank-1 downdating methods based on LINPACK, Corrected Semi-Normal Equations (CSNE), and Gram-Schmidt factorizations, as well as new rank-k downdating methods, can all be derived from these fundamental results. We then analyze the cost of each new algorithm and make comparisons tok applications of the corresponding rank-1 algorithms. We provide experimental results comparing the numerical accuracy of the various algorithms, paying particular attention to the downdating methods, due to their potential numerical difficulties for ill-conditioned problems. We then discuss the computation involved for each downdating method, measured in terms of operation counts and BLAS calls. Finally, we provide serial execution timing results for these algorithms, noting preferable points for improvement and optimization. From our experiments we conclude that the Gram-Schmidt methods perform best in terms of numerical accuracy, but may be too costly for serial execution for large problems.},
issn={1572-9265},
doi={10.1007/BF02140689},
url={https://doi.org/10.1007/BF02140689}
}

@article {pratola_etal.2014,
    AUTHOR = {Pratola, Matthew T. and Chipman, Hugh A. and Gattiker, James
              R. and Higdon, David M. and McCulloch, Robert and Rust,
              William N.},
     TITLE = {Parallel {B}ayesian additive regression trees},
   JOURNAL = {J. Comput. Graph. Statist.},
  FJOURNAL = {Journal of Computational and Graphical Statistics},
    VOLUME = {23},
      YEAR = {2014},
    NUMBER = {3},
     PAGES = {830--852},
      ISSN = {1061-8600},
   MRCLASS = {62J02 (62F15)},
  MRNUMBER = {3224658},
       URL = {https://doi.org/10.1080/10618600.2013.841584},
}
		

@article {chipman_etal.2010,
    AUTHOR = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert
              E.},
     TITLE = {B{ART}: {B}ayesian additive regression trees},
   JOURNAL = {Ann. Appl. Stat.},
  FJOURNAL = {The Annals of Applied Statistics},
    VOLUME = {4},
      YEAR = {2010},
    NUMBER = {1},
     PAGES = {266--298},
      ISSN = {1932-6157},
   MRCLASS = {Expansion},
  MRNUMBER = {2758172},
       URL = {https://doi.org/10.1214/09-AOAS285},
}

@article {denison_etal.1998,
    AUTHOR = {Denison, David G. T. and Mallick, Bani K. and Smith, Adrian F.
              M.},
     TITLE = {A {B}ayesian {CART} algorithm},
   JOURNAL = {Biometrika},
  FJOURNAL = {Biometrika},
    VOLUME = {85},
      YEAR = {1998},
    NUMBER = {2},
     PAGES = {363--377},
      ISSN = {0006-3444},
   MRCLASS = {62H30},
  MRNUMBER = {1649118},
       DOI = {10.1093/biomet/85.2.363},
       URL = {https://doi.org/10.1093/biomet/85.2.363},
}
		
@article{chipman_etal.1998,
author = { Hugh A.   Chipman  and  Edward I.   George  and  Robert E.   McCulloch },
title = {Bayesian CART Model Search},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
volume = {93},
number = {443},
pages = {935-948},
year  = {1998},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1998.10473750},
URL = {https://doi.org/10.1080/01621459.1998.10473750},
eprint = {https://doi.org/10.1080/01621459.1998.10473750}
}

@article{tan_etal.2019,
author = {Tan, Yaoyuan Vincent and Roy, Jason},
title = {Bayesian additive regression trees and the General BART model},
journal = {Statistics in Medicine},
volume = {38},
number = {25},
pages = {5048-5069},
keywords = {Bayesian nonparametrics, Dirichlet process mixtures, machine learning, semiparametric models, spatial},
doi = {https://doi.org/10.1002/sim.8347},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8347},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8347},
year = {2019}
}

@article{parikh_boyd.2014,
author = {Parikh, Neal and Boyd, Stephen},
title = {Proximal Algorithms},
year = {2014},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {1},
number = {3},
issn = {2167-3888},
url = {https://doi.org/10.1561/2400000003},
doi = {10.1561/2400000003},
journal = {Found. Trends Optim.},
month = {jan},
pages = {127–239},
numpages = {113}
}

@article {sun_etal.2017,
    AUTHOR = {Sun, Ying and Babu, Prabhu and Palomar, Daniel P.},
     TITLE = {Majorization-minimization algorithms in signal processing,
              communications, and machine learning},
   JOURNAL = {IEEE Trans. Signal Process.},
  FJOURNAL = {IEEE Transactions on Signal Processing},
    VOLUME = {65},
      YEAR = {2017},
    NUMBER = {3},
     PAGES = {794--816},
      ISSN = {1053-587X,1941-0476},
   MRCLASS = {94A12},
  MRNUMBER = {3580079},
       DOI = {10.1109/TSP.2016.2601299},
       URL = {https://doi.org/10.1109/TSP.2016.2601299},
}
  
@article{bach_etal.2011,
url = {http://dx.doi.org/10.1561/2200000015},
year = {2012},
volume = {4},
journal = {Foundations and Trends® in Machine Learning},
title = {Optimization with Sparsity-Inducing Penalties},
doi = {10.1561/2200000015},
issn = {1935-8237},
number = {1},
pages = {1-106},
author = {Francis Bach and Rodolphe Jenatton and Julien Mairal and Guillaume Obozinski}
}

@book {beck.2017,
    AUTHOR = {Beck, Amir},
     TITLE = {First-order methods in optimization},
    SERIES = {MOS-SIAM Series on Optimization},
    VOLUME = {25},
 PUBLISHER = {Society for Industrial and Applied Mathematics (SIAM),
              Philadelphia, PA; Mathematical Optimization Society,
              Philadelphia, PA},
      YEAR = {2017},
     PAGES = {xii+475},
      ISBN = {978-1-611974-98-0},
   MRCLASS = {90-02 (49M20 65K05 90C06 90C25)},
  MRNUMBER = {3719240},
MRREVIEWER = {Sven-\AA ke\ Gustafson},
       DOI = {10.1137/1.9781611974997.ch1},
       URL = {https://doi.org/10.1137/1.9781611974997.ch1},
}

@article{hesterberg_etal.2008,
author = {Tim Hesterberg and Nam Hee Choi and Lukas Meier and Chris Fraley},
title = {{Least angle and $\ell_1$ penalized regression: A review}},
volume = {2},
journal = {Statistics Surveys},
number = {none},
publisher = {Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
pages = {61 -- 93},
keywords = {ℓ_1 penalty, Lasso, regression, regularization, Variable selection},
year = {2008},
doi = {10.1214/08-SS035},
URL = {https://doi.org/10.1214/08-SS035}
}

@article{gelman.2006,
author = {Andrew Gelman},
title = {{Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper)}},
volume = {1},
journal = {Bayesian Analysis},
number = {3},
publisher = {International Society for Bayesian Analysis},
pages = {515 -- 534},
keywords = {Bayesian inference, conditional conjugacy, folded-noncentral-$t$ distribution, half-$t$ distribution, hierarchical model, multilevel model, noninformative prior distribution, weakly informative prior distribution},
year = {2006},
doi = {10.1214/06-BA117A},
URL = {https://doi.org/10.1214/06-BA117A}
}

@ARTICLE{strimenopoulou_etal.2008,
  title    = "Empirical Bayes logistic regression",
  author   = "Strimenopoulou, Foteini and Brown, Philip J",
  abstract = "We construct a diagnostic predictor for patient disease status
              based on a single data set of mass spectra of serum samples
              together with the binary case-control response. The model is
              logistic regression with Bernoulli log-likelihood augmented
              either by quadratic ridge or absolute L1 penalties. For ridge
              penalization using the singular value decomposition we reduce the
              number of variables for maximization to the rank of the design
              matrix. With log-likelihood loss, 10-fold cross-validatory choice
              is employed to specify the penalization hyperparameter.
              Predictive ability is judged on a set-aside subset of the data.",
  journal  = "Stat Appl Genet Mol Biol",
  volume   =  7,
  number   =  2,
  pages    = "Article9",
  month    =  "feb",
  year     =  2008,
  address  = "Germany",
  language = "en"
}   

@article {brocker_smith.2007,
      author = "Jochen  Bröcker and Leonard A.  Smith",
      title = "Scoring probabilistic forecasts: the Importance of being proper",
      journal = "Weather and Forecasting",
      year = "2007",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "22",
      number = "2",
      doi = "10.1175/WAF966.1",
      pages=      "382 - 388",
      url = "https://journals.ametsoc.org/view/journals/wefo/22/2/waf966_1.xml"
}

@article{gneiting_raftery.2007,
author = {Tilmann Gneiting and Adrian E Raftery},
title = {Strictly proper scoring rules, prediction, and estimation},
   JOURNAL = {J. Amer. Statist. Assoc.},
  FJOURNAL = {Journal of the American Statistical Association},
volume = {102},
number = {477},
pages = {359--378},
year = {2007},
publisher = {ASA Website},
doi = {10.1198/016214506000001437},
URL = {https://doi.org/10.1198/016214506000001437},
eprint = { https://doi.org/10.1198/016214506000001437}
}

@inbook{bai_etal_2021,
   title={Spike-and-Slab Meets LASSO: A Review of the Spike-and-Slab LASSO},
   ISBN={9781003089018},
   url={http://dx.doi.org/10.1201/9781003089018-4},
   DOI={10.1201/9781003089018-4},
   booktitle={Handbook of Bayesian Variable Selection},
   publisher={Chapman and Hall/CRC},
   author={Bai, Ray and Ročková, Veronika and George, Edward I.},
   year={2021},
   month={dec}, 
   pages={81–108}
}

@article {plackett.1950,
    AUTHOR = {Plackett, R. L.},
     TITLE = {Some theorems in least squares},
   JOURNAL = {Biometrika},
  FJOURNAL = {Biometrika},
    VOLUME = {37},
      YEAR = {1950},
     PAGES = {149--157},
      ISSN = {0006-3444,1464-3510},
   MRCLASS = {62.0X},
  MRNUMBER = {36980},
MRREVIEWER = {Benjamin\ Epstein},
       DOI = {10.1093/biomet/37.1-2.149},
       URL = {https://doi.org/10.1093/biomet/37.1-2.149},
}

@article {barbieri_berger.2004,
    AUTHOR = {Barbieri, Maria Maddalena and Berger, James O.},
     TITLE = {Optimal predictive model selection},
   JOURNAL = {Ann. Statist.},
  FJOURNAL = {The Annals of Statistics},
    VOLUME = {32},
      YEAR = {2004},
    NUMBER = {3},
     PAGES = {870--897},
      ISSN = {0090-5364,2168-8966},
   MRCLASS = {62C05 (62C10 62J05)},
  MRNUMBER = {2065192},
       DOI = {10.1214/009053604000000238},
       URL = {https://doi.org/10.1214/009053604000000238},
}

@article {golub_uhlig.2009,
    AUTHOR = {Golub, Gene and Uhlig, Frank},
     TITLE = {The {$QR$} algorithm: 50 years later its genesis by {J}ohn
              {F}rancis and {V}era {K}ublanovskaya and subsequent
              developments},
   JOURNAL = {IMA J. Numer. Anal.},
  FJOURNAL = {IMA Journal of Numerical Analysis},
    VOLUME = {29},
      YEAR = {2009},
    NUMBER = {3},
     PAGES = {467--485},
      ISSN = {0272-4979,1464-3642},
   MRCLASS = {65F15 (01A70 65-03)},
  MRNUMBER = {2520155},
MRREVIEWER = {David\ Scott\ Watkins},
       DOI = {10.1093/imanum/drp012},
       URL = {https://doi.org/10.1093/imanum/drp012},
}

@article {bojanczyk_etal.1993,
    AUTHOR = {Boja\'nczyk, Adam W. and Nagy, James G. and Plemmons, Robert
              J.},
     TITLE = {Block {RLS} using row {H}ouseholder reflections},
   JOURNAL = {Linear Algebra Appl.},
  FJOURNAL = {Linear Algebra and its Applications},
    VOLUME = {188/189},
      YEAR = {1993},
     PAGES = {31--61},
      ISSN = {0024-3795,1873-1856},
   MRCLASS = {65F25 (15A23 65F20)},
  MRNUMBER = {1223456},
MRREVIEWER = {Alfonso\ Laratta},
       DOI = {10.1016/0024-3795(93)90464-Y},
       URL = {https://doi.org/10.1016/0024-3795(93)90464-Y},
}

@article {gill_etal.1974,
    AUTHOR = {Gill, P. E. and Golub, G. H. and Murray, W. and Saunders, M.
              A.},
     TITLE = {Methods for modifying matrix factorizations},
   JOURNAL = {Math. Comp.},
  FJOURNAL = {Mathematics of Computation},
    VOLUME = {28},
      YEAR = {1974},
     PAGES = {505--535},
      ISSN = {0025-5718,1088-6842},
   MRCLASS = {65F05},
  MRNUMBER = {343558},
MRREVIEWER = {L.\ W.\ Ehrlich},
       DOI = {10.2307/2005923},
       URL = {https://doi.org/10.2307/2005923},
}

@article {hager.1989,
    AUTHOR = {Hager, William W.},
     TITLE = {Updating the inverse of a matrix},
   JOURNAL = {SIAM Rev.},
  FJOURNAL = {SIAM Review. A Publication of the Society for Industrial and
              Applied Mathematics},
    VOLUME = {31},
      YEAR = {1989},
    NUMBER = {2},
     PAGES = {221--239},
      ISSN = {0036-1445},
   MRCLASS = {65Fxx},
  MRNUMBER = {997457},
       DOI = {10.1137/1031049},
       URL = {https://doi.org/10.1137/1031049},
}

@article {wei_etal.2020,
    AUTHOR = {Wei, Wei and Dai, Hua and Liang, Weitai},
     TITLE = {Block updating/downdating algorithms for regularised least
              squares problems and applications to linear discriminant
              analysis},
   JOURNAL = {East Asian J. Appl. Math.},
  FJOURNAL = {East Asian Journal on Applied Mathematics},
    VOLUME = {10},
      YEAR = {2020},
    NUMBER = {4},
     PAGES = {679--697},
      ISSN = {2079-7362,2079-7370},
   MRCLASS = {65F10},
  MRNUMBER = {4146028},
MRREVIEWER = {Dimitrios\ Christou},
       DOI = {10.4208/eajam.171219.220220},
       URL = {https://doi.org/10.4208/eajam.171219.220220},
}

@article {elden_park.1994,
    AUTHOR = {Eld\'en, L. and Park, H.},
     TITLE = {Block downdating of least squares solutions},
   JOURNAL = {SIAM J. Matrix Anal. Appl.},
  FJOURNAL = {SIAM Journal on Matrix Analysis and Applications},
    VOLUME = {15},
      YEAR = {1994},
    NUMBER = {3},
     PAGES = {1018--1034},
      ISSN = {0895-4798},
   MRCLASS = {65F20},
  MRNUMBER = {1282711},
MRREVIEWER = {Mei-Qin\ Chen},
       DOI = {10.1137/S089547989223691X},
       URL = {https://doi.org/10.1137/S089547989223691X},
}

@article{khan_etal.2024,
title = {A review of ensemble learning and data augmentation models for class imbalanced problems: Combination, implementation and evaluation},
journal = {Expert Systems with Applications},
volume = {244},
pages = {122778},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122778},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423032803},
author = {Azal Ahmad Khan and Omkar Chaudhari and Rohitash Chandra},
keywords = {Class imbalance, Machine learning, Data augmentation, Ensemble learning},
abstract = {Class imbalance (CI) in classification problems arises when the number of observations belonging to one class is lower than the other. Ensemble learning combines multiple models to obtain a robust model and has been prominently used with data augmentation methods to address class imbalance problems. In the last decade, a number of strategies have been added to enhance ensemble learning and data augmentation methods, along with new methods such as generative adversarial networks (GANs). A combination of these has been applied in many studies, and the evaluation of different combinations would enable a better understanding and guidance for different application domains. In this paper, we present a computational study to evaluate data augmentation and ensemble learning methods used to address prominent benchmark CI problems. We present a general framework that evaluates 9 data augmentation and 9 ensemble learning methods for CI problems. Our objective is to identify the most effective combination for improving classification performance on imbalanced datasets. The results indicate that combinations of data augmentation methods with ensemble learning can significantly improve classification performance on imbalanced datasets. We find that traditional data augmentation methods such as the synthetic minority oversampling technique (SMOTE) and random oversampling (ROS) are not only better in performance for selected CI problems, but also computationally less expensive than GANs. Our study is vital for the development of novel models for handling imbalanced datasets.}
}

@book {ruppert_etal.2003,
    AUTHOR = {Ruppert, David and Wand, M. P. and Carroll, R. J.},
     TITLE = {Semiparametric {R}egression},
    SERIES = {Cambridge Series in Statistical and Probabilistic Mathematics},
    VOLUME = {12},
 PUBLISHER = {Cambridge University Press, Cambridge},
      YEAR = {2003},
     PAGES = {xvi+386},
      ISBN = {0-521-78050-0; 0-521-78516-2},
   MRCLASS = {62-02 (62G05 62G08)},
  MRNUMBER = {1998720},
MRREVIEWER = {Jack\ Cuzick},
       DOI = {10.1017/CBO9780511755453},
       URL = {https://doi.org/10.1017/CBO9780511755453},
}

@article {tibshirani.2015,
    AUTHOR = {Tibshirani, Ryan J.},
     TITLE = {A general framework for fast stagewise algorithms},
   JOURNAL = {J. Mach. Learn. Res.},
  FJOURNAL = {Journal of Machine Learning Research (JMLR)},
    VOLUME = {16},
      YEAR = {2015},
     PAGES = {2543--2588},
      ISSN = {1532-4435,1533-7928},
   MRCLASS = {62J07 (62L12)},
  MRNUMBER = {3450516},
MRREVIEWER = {Gabriela\ Ciuperca},
}

@article {wood.2008,
    AUTHOR = {Wood, Simon N.},
     TITLE = {Fast stable direct fitting and smoothness selection for
              generalized additive models},
   JOURNAL = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  FJOURNAL = {Journal of the Royal Statistical Society. Series B.
              Statistical Methodology},
    VOLUME = {70},
      YEAR = {2008},
    NUMBER = {3},
     PAGES = {495--518},
      ISSN = {1369-7412,1467-9868},
   MRCLASS = {99-01},
  MRNUMBER = {2420412},
       DOI = {10.1111/j.1467-9868.2007.00646.x},
       URL = {https://doi.org/10.1111/j.1467-9868.2007.00646.x},
}

@article{zeugner_feldkircher.2015,
 title={Bayesian model averaging employing fixed and flexible priors: the BMS package for R},
 volume={68},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v068i04},
 doi={10.18637/jss.v068.i04},
 abstract={This article describes the BMS (Bayesian model sampling) package for R that implements Bayesian model averaging for linear regression models. The package excels in allowing for a variety of prior structures, among them the &amp;quot;binomial-beta&amp;quot; prior on the model space and the so-called &amp;quot;hyper-g&amp;quot; specifications for Zellner’s g prior. Furthermore, the BMS package allows the user to specify her own model priors and offers a possibility of subjective inference by setting &amp;quot;prior inclusion probabilities&amp;quot; according to the researcher’s beliefs. Furthermore, graphical analysis of results is provided by numerous built-in plot functions of posterior densities, predictive densities and graphical illustrations to compare results under different prior settings. Finally, the package provides full enumeration of the model space for small scale problems as well as two efficient MCMC (Markov chain Monte Carlo) samplers that sort through the model space when the number of potential covariates is large.},
 number={4},
 journal={Journal of Statistical Software},
 author={Zeugner, Stefan and Feldkircher, Martin},
 year={2015},
 pages={1–37}
}